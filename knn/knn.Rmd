---
title: "Competicion Kaggle Knn"
author: "Carmen Biedma Rodriguez"
date: "4 de febrero de 2019"
output: pdf_document
---

```{r}
library(dplyr)
library(Hmisc)
library(caret)
library(mice)
library(VIM)
library(class)
library(outliers)
library(ggplot2)
library(mvoutlier)
library(FSelector)
library(corrplot)

source("../funcionesAux.R")
```

```{r}
datos <- read.csv("../train.csv",na.strings=c(" ","NA","?"))
test <- read.csv("../test.csv",na.strings=c(" ","NA","?"))
```

#Visi?n preliminar de los datos

```{r}
nrow(datos)
ncol(datos)
```

```{r}
summary(datos)
```

Como podemos ver, es un problema que tiene 50 variables descriptivas y una de clasificaci?n. Una de las cosas m?s importantes que nos proporciona este resumen de los datos es que todas las varables tienen valores perdidos, por lo que ser? un problema a solucionar. Por otro lado, podemos ver que los dominios de las variables son muy diferentes unos de otros por lo que tambi?n convendr? aplicar un centrado y escalado. ?sto ?ltimo es muy importante ya que vamos a aplicar un m?todo basado en distancias.

Otra de las cosas que ser?a interesante observar es si tenemos un problema desbalanceado, para ello crearemos una tabla en la que veremos las instancias que hay de una clase y de otra.

```{r}
table(datos$C)
```

Como podemos ver, es un problema desbalanceado, por lo que ser? un aspecto a tratar en el preprocesamiento de los datos.


#ESCALADO Y CENTRADO DE LOS DATOS

```{r}
datosPrep <- caret::preProcess(datos[,-ncol(datos)],method = c("center","scale"))
datos[,-ncol(datos)] <- predict(datosPrep,datos[,-ncol(datos)])
datosPrep <- caret::preProcess(test,method = c("center","scale"))
test <- predict(datosPrep,test)
```



#Imputaci?n de valores perdidos.

En primer lugar, vamos a eliminar las instancias qeu tienen un porcentaje de valores peridos superior aun umbral previamente establecido.

En primer lugar, con el siguiente comando, calcularemos el porcentaje de valores perdidos que tiene cada fila. 


```{r}
porcentajes <- apply(datos,1, function(x) sum(is.na(x))) / ncol(datos) * 100
eliminar <- porcentajes>5
which(eliminar == TRUE)
```

Como vemos, no hay ninguna instancia que podamos eliminar siguiendo este criterio, por lo que tendremos que realizar otra tarea para eliminar los valores perdidos.

A continuaci?n se imputar?n los valores perdidos. Esto quiere decir que a cada uno de ellos se le asignar? unn valor. Dicho valor puede ser el mismo para todos, la media o cualquier otra medida basada en los dem?s datos de la variable o un valor proporcionado por cualquier m?todo de imputaci?n.

Vamos a ver el patr?n de datos perdidos que sigue nuestro conjunto de datos usando la libreria mice. Como podemos ver a continuaci?n, tenemos un total de 7778 variables que no tienen ningun valor perdido.

```{r}
patron <- mice::md.pattern(datos)
print(patron)
```

Si observamos gr?ficamente el resultado proporcionado por el patr?n, vemos que no hay ninguna variable que tenga m?s de un valor perdido. Esto explica que en el paso anterior no se haya eliminado ninguna instancia, ya que, que una de las variables est? perdida no es raz?n suficiente como para poder eliminar la fila conmpleta.

Hay otra libreria que nos permite visualizar de distintas formas los valores perdidos de un conjunto de datos. Veamos a continuaci?n las utilidades que nos proporciona.

```{r}
plot <- VIM::aggr(datos,col=c('blue','red'),numbers=TRUE,sortVars=TRUE,labels=names(data),
                  cex.axis=.5,gap=1,ylab=c("Gr?fico de datos perdidos","Patron"))
```

A continuaci?n se procede a realizar la imputaci?n:________________________--

```{r}
completos <- mice::ncc(datos)
incompletos <- mice::nic(datos)
cat("COMPLETOS = ", completos, " INCOMPLETOS = ",incompletos)
imputados <- mice::mice(datos,m=5,meth="pmm")

datos.imputados <- mice::complete(imputados)

completos <- mice::ncc(datos.imputados)
incompletos <- mice::nic(datos.imputados)
cat("COMPLETOS = ", completos, " INCOMPLETOS = ",incompletos)

```

Con el m?todo que se ha usado, se realizan 5 imputaciones distintas, de las cuales eligiremos la que mejor se ajuste a nuestros datos. Para elegir cu?l de ellas utilizaremos, nos quedaremos con la que se distribuya de forma m?s parecida a los datos originales. En el siguiente gr?fico de densidad podemos ver para cada una de las variables imputadas la distribucion que siguen con respecto a la densidad.


Como vemos, el m?todo pmm de mice no imputa todosl os valores, aplicaremos amelia que no tiene problema para imputarlos todos.

```{r}
imputados.amelia <- Amelia::amelia(datos,m=1,parallel="multicore",noms="C")
incompletos.amelia <- mice::nic(imputados.amelia$imputations$imp1)
completos.amelia <- mice::ncc(imputados.amelia$imputations$imp1)
cat("COMPLETOS = ", completos.amelia, " INCOMPLETOS = ",incompletos.amelia)
datos <- imputados.amelia$imputations$imp1
```

#Detecci?n de datos an?malos


```{r}
anomalos <- outliers::outlier(datos[,-ncol(datos)])
print(anomalos)
```

Al m?todo siguiente solo se le puede pasar un argumento de menos de 10 dimensiones, por lo que si queremos ver todos los outliers que hay en todas las variables, tenemos que ponerals por separado. Adem?s, solo considera los outliers de una dimensi?n, es decir, que no detecta outliers qeu est?n provocados por una combiaci?n an?mala de valores de un conjhunto de variables.

```{r}
resOutliers <- mvoutlier::uni.plot(datos[1:10])
```

```{r}
print(resOutliers$outliers)
```

```{r}
datos <- datos[!resOutliers$outliers, ]
```


##Selecci?n de caracteristicas

La selecci?n de caracter?sticas es una t?cnica para reducir la diimensionalidad muy utilizada. Cuando realizamos esta tares obtenemos modelos m?s r?pidos (hay menos datos), con mayor exactitud ya que la generalizaci?n es mayor y obtenemos unos resultados m?s simples.

Para realizar esta tarea hay diferentes criterios y diferentes algoritmos. Vamos a probar con algunos de ellos para ver qu? selecci?n funciona mejor para el algoritmo de estudio. En primer lugar se aplicar?n medidas para eliminar variables y a continuaci?n veremos si con alg?n algoritmo m?s completo podemos obtener mejores resultados.

#Correlaci?n lineal

EL MODELO OBTENIDO NOS DA UN ACCURACY EN TRAIN DE 0.9995869 Y EN TEST DE 0.79989 POR LO QUE PODEMOS DECIR QUE EL MODELO SOBREAJUSTA. -> NOTA: Habr?a que ver si hay que eliminar menos variables o m?s.

En primer lugar, vamos a estudiar la importancia de las variables que teemos en el problema. Para ello, en primera instancia, veremos como de correladas est?n entre s?. Eliminaremos aquiellas que est?n m?s correladas.


```{r}
y = cor(datos)
corrplot(y,type="upper")
```

Para ver la importancia de las variables en relaci?n a dicha medida vamos a usar el paquete FSelector. En primer lugar calculamos los pesos de todas las variables y despu?s me quedar? con las 20 mejores.

```{r}
pesosCorrelacion <- FSelector::linear.correlation(C~., datos)
print(pesosCorrelacion)
```

```{r}
subset.correlation <- FSelector::cutoff.k(pesosCorrelacion,20)
f <- as.simple.formula(subset.correlation,"medv")
print(f)
datos1.correlation <- datos[,subset.correlation]
datos1.correlation$C <- datos$C
test1.correlation <- test[,subset.correlation]
```

#chi.squared

En este caso, se utiliza el test chi cuadrado para obtener la relevancia de las variables.

```{r}
pesosChi.squared <- FSelector::chi.squared(C~., datos)
```

```{r}
subset.chisquared <- FSelector::cutoff.k(pesosChi.squared,20)
datos2.chisquared <- datos[,subset.chisquared]
datos2.chisquared$C <- datos$C
test2.chisquared <- test[,subset.chisquared]
```



#entropy.based

```{r}
pesosEntropy.based <- FSelector::information.gain(C~., datos)
```

```{r}
subset.entropybased<- FSelector::cutoff.k(pesosEntropy.based,20)
datos3.entropybased <- datos[,subset.entropybased]
datos3.entropybased$C <- datos$C
test3.entropybased <- test[,subset.entropybased]
```

# oneR


```{r}
pesosOneR <- FSelector::oneR(C~., datos)
```

```{r}
subset.oneR<- FSelector::cutoff.k(pesosOneR,20)
datos4.oneR <- datos[,subset.oneR]
datos4.oneR$C <- datos$C
test4.oneR <- test[,subset.oneR]
```

#Random forest

(copiado del pdf) El tercer argumento puede tener valor 1 ? 2, e indica el tipo de medida de importancia a
usar: 1 (reducci?n en la media de fiabilidad predictiva) y 2 (reducci?n en la media de impureza
de nodos).

```{r}
pesosRandomForest <- FSelector::random.forest.importance(C~.,datos, importance.type=1)
```

```{r}
subset.RandomForest<- FSelector::cutoff.k(pesosRandomForest,20)
datos5.RandomForest <- datos[,subset.RandomForest]
datos5.RandomForest$C <- datos$C
test5.RandomForest <- test[,subset.RandomForest]
```

#Resultados 


```{r}
selecciones <- data.frame(correlacion.lineal = subset.correlation,
                          chi.squared = subset.chisquared,
                          entropy.based = subset.entropybased,
                          oneR = subset.oneR,
                          randomForest = subset.RandomForest)
selecciones
```

##Generaci?n del modelo________________________________________________________________

#1- Modelo con selecci?n de variables por correlacion

```{r}
generarModelo <- function(train,test,k=3){
  pred  <- knn(train = train[,-ncol(train)], test = test,cl = train$C, k)
  pred
}
```

Accuracy para los datos de train.

```{r}
train_pred.correlation <- generarModelo(train = datos1.correlation, test = datos1.correlation[,-ncol(datos1.correlation)],k=3)
accuracy.correlation <- precision(train_pred.correlation,datos1.correlation$C)
accuracy.correlation
```

C?lculo de etiquetas del test

```{r}
test_pred.correlation <- train_pred <- generarModelo(train = datos1.correlation, test = test1.correlation,3)
```


```{r}
generarEnvio(y=test_pred.correlation,file="envio.csv")
```


#2- Modelo con selecci?n de variables por chi.squared

Accuracy para los datos de train.

```{r}
train_pred.chisquared <- generarModelo(train = datos2.chisquared, test = datos2.chisquared[,-ncol(datos2.chisquared)],k=3)
accuracy.chisquared <- precision(train_pred.chisquared,datos2.chisquared$C)
accuracy.chisquared
```

C?lculo de etiquetas del test

```{r}
test_pred.chisquared <- generarModelo(train = datos2.chisquared, test = test2.chisquared,3)
```


```{r}
generarEnvio(y=test_pred.chisquared,file="envio.csv")
```


#3- Modelo con selecci?n de variables por entropia

Por ahora no aplicar? este modelo porque difiere en uan sola varibale de la selecci?n hecha con correlaci?n, por lo que no creo que los resultados cambien mucho.z

#4- Modelo con selecci?n de variables por oneR

Accuracy para los datos de train.

```{r}
train_pred.oneR <- generarModelo(train = datos4.oneR, test = datos4.oneR[,-ncol(datos4.oneR)],k=3)
accuracy.oneR<- precision(train_pred.oneR,datos4.oneR$C)
accuracy.oneR
```

C?lculo de etiquetas del test

```{r}
test_pred.oneR <- generarModelo(train = datos4.oneR, test = test4.oneR,3)
```



```{r}
generarEnvio(y=test_pred.oneR,file="envio.csv")
```


#4- Modelo con selecci?n de variables por oneR

Accuracy para los datos de train.

```{r}
train_pred.RandomForest <- generarModelo(train = datos5.RandomForest, test = datos5.RandomForest[,-ncol(datos5.RandomForest)],k=3)
accuracy.RandomForest<- precision(train_pred.RandomForest,datos5.RandomForest$C)
accuracy.RandomForest
```


C?lculo de etiquetas del test

```{r}
test_pred.RandomForest <- generarModelo(train = datos5.RandomForest, test = test5.RandomForest,3)
```



```{r}
generarEnvio(y=test_pred.RandomForest,file="envio.csv")
```

#RESULTADOS



```{r}
resultados <- data.frame(modelo = c("correlacion","chisquared","oneR","RandomForest"),
                         accuracy.train = c(accuracy.correlation,accuracy.chisquared,accuracy.oneR,
                                            accuracy.RandomForest),
                         accuracy.test = c(0.79938,0.81419,0.81725,0.81214))
resultados
```