---
title: "Competicion Kaggle Knn"
author: "Carmen Biedma Rodriguez"
date: "4 de febrero de 2019"
output: pdf_document
---

```{r}
library(dplyr)
library(Hmisc)
library(caret)
library(mice)
library(VIM)
library(class)
library(outliers)
library(ggplot2)
library(mvoutlier)
library(FSelector)
library(corrplot)
library(NoiseFiltersR)

source("../funcionesAux.R")

generarModelo <- function(train,test,k=3){
  pred  <- knn(train = train[,-ncol(train)], test = test,cl = train$C, k)
  pred
}
```

```{r}
datos <- read.csv("../train.csv",na.strings=c(" ","NA","?"))
test <- read.csv("../test.csv",na.strings=c(" ","NA","?"))
```

#Visi?n preliminar de los datos

```{r}
nrow(datos)
ncol(datos)
```

```{r}
summary(datos)
```

Como podemos ver, es un problema que tiene 50 variables descriptivas y una de clasificaci?n. Una de las cosas m?s importantes que nos proporciona este resumen de los datos es que todas las varables tienen valores perdidos, por lo que ser? un problema a solucionar. Por otro lado, podemos ver que los dominios de las variables son muy diferentes unos de otros por lo que tambi?n convendr? aplicar un centrado y escalado. ?sto ?ltimo es muy importante ya que vamos a aplicar un m?todo basado en distancias.

Otra de las cosas que ser?a interesante observar es si tenemos un problema desbalanceado, para ello crearemos una tabla en la que veremos las instancias que hay de una clase y de otra.

```{r}
ggplot(datos, aes(C)) +
  geom_histogram(binwidth = 1)
```

Como podemos ver, es un problema desbalanceado, por lo que ser? un aspecto a tratar en el preprocesamiento de los datos.

```{r}
boxplot(datos[1:25])
boxplot(datos[26:49])
```


#ESCALADO Y CENTRADO DE LOS DATOS

```{r}
datosPrep <- caret::preProcess(datos[,-ncol(datos)],method = c("center","scale"))
datos[,-ncol(datos)] <- predict(datosPrep,datos[,-ncol(datos)])
test <- predict(datosPrep,test)
```

  

#Imputación de valores perdidos.

En primer lugar, vamos a eliminar las instancias qeu tienen un porcentaje de valores peridos superior aun umbral previamente establecido.

En primer lugar, con el siguiente comando, calcularemos el porcentaje de valores perdidos que tiene cada fila. 


```{r}
porcentajes <- apply(datos,1, function(x) sum(is.na(x))) / ncol(datos) * 100
eliminar <- porcentajes>5
which(eliminar == TRUE)
```

Como vemos, no hay ninguna instancia que podamos eliminar siguiendo este criterio, por lo que tendremos que realizar otra tarea para eliminar los valores perdidos.

A continuaci?n se imputar?n los valores perdidos. Esto quiere decir que a cada uno de ellos se le asignar? unn valor. Dicho valor puede ser el mismo para todos, la media o cualquier otra medida basada en los dem?s datos de la variable o un valor proporcionado por cualquier m?todo de imputaci?n.

Vamos a ver el patr?n de datos perdidos que sigue nuestro conjunto de datos usando la libreria mice. Como podemos ver a continuaci?n, tenemos un total de 7778 variables que no tienen ningun valor perdido.

 

Si observamos gr?ficamente el resultado proporcionado por el patr?n, vemos que no hay ninguna variable que tenga m?s de un valor perdido. Esto explica que en el paso anterior no se haya eliminado ninguna instancia, ya que, que una de las variables est? perdida no es raz?n suficiente como para poder eliminar la fila conmpleta.

Hay otra libreria que nos permite visualizar de distintas formas los valores perdidos de un conjunto de datos. Veamos a continuaci?n las utilidades que nos proporciona.

```{r}
plot <- VIM::aggr(datos,col=c('blue','red'),numbers=TRUE,sortVars=TRUE,labels=names(data),
                  cex.axis=.5,gap=1,ylab=c("Gr?fico de datos perdidos","Patron"))
```

A continuaci?n se procede a realizar la imputaci?n:________________________--

```{r}
completos <- mice::ncc(datos)
incompletos <- mice::nic(datos)
cat("COMPLETOS = ", completos, " INCOMPLETOS = ",incompletos)
imputados <- mice::mice(datos,m=5,meth="pmm")

datos.imputados <- mice::complete(imputados)

completos <- mice::ncc(datos.imputados)
incompletos <- mice::nic(datos.imputados)
cat("COMPLETOS = ", completos, " INCOMPLETOS = ",incompletos)

```

Con el m?todo que se ha usado, se realizan 5 imputaciones distintas, de las cuales eligiremos la que mejor se ajuste a nuestros datos. Para elegir cu?l de ellas utilizaremos, nos quedaremos con la que se distribuya de forma m?s parecida a los datos originales. En el siguiente gr?fico de densidad podemos ver para cada una de las variables imputadas la distribucion que siguen con respecto a la densidad.


Como vemos, el m?todo pmm de mice no imputa todosl os valores, aplicaremos amelia que no tiene problema para imputarlos todos.

```{r}
imputados.amelia <- Amelia::amelia(datos,m=1,parallel="multicore",noms="C")
incompletos.amelia <- mice::nic(imputados.amelia$imputations$imp1)
completos.amelia <- mice::ncc(imputados.amelia$imputations$imp1)
cat("COMPLETOS = ", completos.amelia, " INCOMPLETOS = ",incompletos.amelia)
datos <- imputados.amelia$imputations$imp1
```

#Detección de datos anómalos


```{r}
anomalos <- outliers::outlier(datos[,-ncol(datos)])
print(anomalos)
```

Al método siguiente solo se le puede pasar un argumento de menos de 10 dimensiones, por lo que si queremos ver todos los outliers que hay en todas las variables, tenemos que ponerals por separado. Además, solo considera los outliers de una dimensión, es decir, que no detecta outliers qeu estén provocados por una combiación anómala de valores de un conjhunto de variables.

```{r}
resOutliers <- mvoutlier::uni.plot(datos[1:10])
```

```{r}
datos <- datos[!resOutliers$outliers, ]
```


##Selección de caracteristicas

La selección de características es una técnica para reducir la diimensionalidad muy utilizada. Cuando realizamos esta tares obtenemos modelos más rápidos (hay menos datos), con mayor exactitud ya que la generalización es mayor y obtenemos unos resultados más simples.

Para realizar esta tarea hay diferentes criterios y diferentes algoritmos. Vamos a probar con algunos de ellos para ver qué selección funciona mejor para el algoritmo de estudio. En primer lugar se aplicarán medidas para eliminar variables y a continuación veremos si con algún algoritmo más completo podemos obtener mejores resultados.

#Correlación lineal


En primer lugar, vamos a estudiar la importancia de las variables que teemos en el problema. Para ello, en primera instancia, veremos como de correladas están entre sí. Eliminaremos aquiellas que están más correladas.


```{r}
y = cor(datos)
corrplot(y,type="upper")
```

Para ver la importancia de las variables en relación a dicha medida vamos a usar el paquete FSelector. En primer lugar calculamos los pesos de todas las variables y después me quedaré con las 20 mejores.

```{r}
pesosCorrelacion <- FSelector::linear.correlation(C~., datos)
```

```{r}
subset.correlation <- FSelector::cutoff.k(pesosCorrelacion,20)
f <- as.simple.formula(subset.correlation,"medv")
print(f)
datos1.correlation <- datos[,subset.correlation]
datos1.correlation$C <- datos$C
test1.correlation <- test[,subset.correlation]
```

#chi.squared

En este caso, se utiliza el test chi cuadrado para obtener la relevancia de las variables.

```{r}
pesosChi.squared <- FSelector::chi.squared(C~., datos)
```

```{r}
subset.chisquared <- FSelector::cutoff.k(pesosChi.squared,20)
datos2.chisquared <- datos[,subset.chisquared]
datos2.chisquared$C <- datos$C
test2.chisquared <- test[,subset.chisquared]
```



#entropy.based

```{r}
pesosEntropy.based <- FSelector::information.gain(C~., datos)
```

```{r}
subset.entropybased<- FSelector::cutoff.k(pesosEntropy.based,20)
datos3.entropybased <- datos[,subset.entropybased]
datos3.entropybased$C <- datos$C
test3.entropybased <- test[,subset.entropybased]
```

# oneR


```{r}
pesosOneR <- FSelector::oneR(C~., datos)
```

```{r}
subset.oneR<- FSelector::cutoff.k(pesosOneR,20)
datos4.oneR <- datos[,subset.oneR]
datos4.oneR$C <- datos$C
test4.oneR <- test[,subset.oneR]
```

#Random forest

(copiado del pdf) El tercer argumento puede tener valor 1 ó 2, e indica el tipo de medida de importancia a
usar: 1 (reducción en la media de fiabilidad predictiva) y 2 (reducción en la media de impureza
de nodos).

```{r}
pesosRandomForest <- FSelector::random.forest.importance(C~.,datos, importance.type=1)
```

```{r}
subset.RandomForest<- FSelector::cutoff.k(pesosRandomForest,20)
datos5.RandomForest <- datos[,subset.RandomForest]
datos5.RandomForest$C <- datos$C
test5.RandomForest <- test[,subset.RandomForest]
```


#Resultados 


```{r}
selecciones <- data.frame(correlacion.lineal = subset.correlation,
                          chi.squared = subset.chisquared,
                          entropy.based = subset.entropybased,
                          oneR = subset.oneR,
                          randomForest = subset.RandomForest)
selecciones
```

##Generación del modelo________________________________________________________________

#1- Modelo con selección de variables por correlacion


Accuracy para los datos de train.

```{r}
train_pred.correlation <- generarModelo(train = datos1.correlation, test = datos1.correlation[,-ncol(datos1.correlation)],k=3)
accuracy.correlation <- precision(train_pred.correlation,datos1.correlation$C)
accuracy.correlation
```

Cálculo de etiquetas del test

```{r}
test_pred.correlation <- train_pred <- generarModelo(train = datos1.correlation, test = test1.correlation,3)
```


```{r}
generarEnvio(y=test_pred.correlation,file="envio.csv")
```


#2- Modelo con selección de variables por chi.squared

Accuracy para los datos de train.

```{r}
train_pred.chisquared <- generarModelo(train = datos2.chisquared, test = datos2.chisquared[,-ncol(datos2.chisquared)],k=3)
accuracy.chisquared <- precision(train_pred.chisquared,datos2.chisquared$C)
accuracy.chisquared
```

Cálculo de etiquetas del test

```{r}
test_pred.chisquared <- generarModelo(train = datos2.chisquared, test = test2.chisquared,3)
```


```{r}
generarEnvio(y=test_pred.chisquared,file="envio.csv")
```


#3- Modelo con selección de variables por entropia

Por ahora no aplicaré este modelo porque difiere en uan sola varibale de la selección hecha con correlación, por lo que no creo que los resultados cambien mucho.z

#4- Modelo con selección de variables por oneR

Accuracy para los datos de train.

```{r}
train_pred.oneR <- generarModelo(train = datos4.oneR, test = datos4.oneR[,-ncol(datos4.oneR)],k=3)
accuracy.oneR<- precision(train_pred.oneR,datos4.oneR$C)
accuracy.oneR
```

Cálculo de etiquetas del test

```{r}
test_pred.oneR <- generarModelo(train = datos4.oneR, test = test4.oneR,3)
```



```{r}
generarEnvio(y=test_pred.oneR,file="envio.csv")
```


#4- Modelo con selección de variables por oneR

Accuracy para los datos de train.

```{r}
train_pred.RandomForest <- generarModelo(train = datos5.RandomForest, test = datos5.RandomForest[,-ncol(datos5.RandomForest)],k=3)
accuracy.RandomForest<- precision(train_pred.RandomForest,datos5.RandomForest$C)
accuracy.RandomForest
```


Cálculo de etiquetas del test

```{r}
test_pred.RandomForest <- generarModelo(train = datos5.RandomForest, test = test5.RandomForest,3)
```



```{r}
generarEnvio(y=test_pred.RandomForest,file="envio.csv")
```

#RESULTADOS DEL ESTUDIO DE SELECCION DE VARIABLES



```{r}
resultados <- data.frame(modelo = c("correlacion","chisquared","oneR","RandomForest"),
                         accuracy.train = c(accuracy.correlation,accuracy.chisquared,accuracy.oneR,
                                            accuracy.RandomForest),
                         accuracy.test = c(0.79938,0.81419,0.81725,0.81214))
resultados
```

CONCLUSION: Me quedaré con el conjunto de variables proporcionadas con oneR ya que es el que mejores resultados da en test.

```{r}
datos <- datos4.oneR
test <- test4.oneR
```


#Eliminación de ruido

En primer lugar, para poder utilizar el método IPF tenemos que pasar a factor la variable de clasificación.

```{r}
datos$C <- as.factor(datos$C)
```

```{r}
set.seed (1)
out <- IPF(C~., data = datos ,s = 3)
summary(out, explicit =TRUE)
out$cleanData
datosSinRuido <- datos[setdiff(1:nrow(datos),out$remIdx),]
```

```{r}
cat("Se han eliminado ", dim(datos)[1]- dim(datosSinRuido)[1], "instancias")
```

```{r}
datos <- datosSinRuido
```

#Generación del modelo sin ruido


Prueba con k = 3
```{r}
train_pred <- generarModelo(train = datos, test = datos[,-ncol(datos)],k=3)
accuracyk3<- precision(train_pred,datos$C)
accuracyk3
```

Cálculo de etiquetas del test

```{r}
test_pred <- generarModelo(train = datos, test = test,3)
```


```{r}
generarEnvio(y=test_pred,file="envio_k3.csv")
```


Prueba con k = 4
```{r}
train_pred <- generarModelo(train = datos, test = datos[,-ncol(datos)],k=4)
accuracyk4<- precision(train_pred,datos$C)
accuracyk4
```

Cálculo de etiquetas del test

```{r}
test_pred <- generarModelo(train = datos, test = test,4)
```


```{r}
generarEnvio(y=test_pred,file="envio_k4.csv")
```


#Resultados sin ruido y probando k=4,k=3

```{r}
resultados <- data.frame(modelo = c("k=3","k=4"),
                         accuracy.train = c(accuracyk3,accuracyk4),
                         accuracy.test = c(0.86421,0.86268))
resultados
```


MODELO SIN ELIMINAR OUTLIERS POR SEPARADO, SOLO CON ELIMINACIÓN DE RUIDO


```{r}
train_pred <- generarModelo(train = datos, test = datos[,-ncol(datos)],k=3)
accuracy<- precision(train_pred,datos$C)
accuracy
```

Cálculo de etiquetas del test

```{r}
test_pred <- generarModelo(train = datos, test = test,3)
```


```{r}
generarEnvio(y=test_pred,file="envio_con_outliers.csv")
```



MODELO SIN ELIMINAR OUTLIERS POR SEPARADO, SOLO CON ELIMINACIÓN DE RUIDO Y CON SELECCION POR CORRELACION

1.Aplicamos escalado
2.imputamos valores
3.Selecion de variables con la correlacion
4. Eliminacion de ruido (suin previa eliminacion de outliers)

```{r}
datos <- datos1.correlation
```


```{r}
datos$C <- as.factor(datos$C)
```

```{r}
set.seed (1)
out <- IPF(C~., data = datos ,s = 3)
summary(out, explicit =TRUE)
out$cleanData
datosSinRuido <- datos[setdiff(1:nrow(datos),out$remIdx),]
```


```{r}
cat("Se han eliminado ", dim(datos)[1]- dim(datosSinRuido)[1], "instancias")
```

```{r}
datos <- datosSinRuido
```

```{r}
corrplot(cor(datos[,-ncol(datos)]))
```

```{r}
cor(datos[,-ncol(datos)])
```

Eliminaremos X26,X21,X15,X37,X24,X17,X32,X30,X7,X20,X47,X13


```{r}
datos.sinCorrelados <- datos[,c("X16","X23","X27","X42","X3","X18","X12","X34","C")]
test.sinCorrelados <- test[,c("X16","X23","X27","X42","X3","X18","X12","X34")]
```


```{r}
corrplot(cor(datos.sinCorrelados[,-ncol(datos.sinCorrelados)]))
```



Generacion del modelo


```{r}
train_pred <- generarModelo(train = datos.sinCorrelados, test = datos.sinCorrelados[,-ncol(datos.sinCorrelados)],k=3)
accuracy<- precision(train_pred,datos.sinCorrelados$C)
accuracy
```

Cálculo de etiquetas del test

```{r}
test_pred <- generarModelo(train = datos.sinCorrelados, test = test.sinCorrelados,3)
```


```{r}
generarEnvio(y=test_pred,file="envio_sin_correlados.csv")
```


#Desbalanceo de datos

Empiezo otra vez de 0 con los datos 


```{r}
datos <- read.csv("../train.csv",na.strings=c(" ","NA","?"))
test <- read.csv("../test.csv",na.strings=c(" ","NA","?"))
```


Imputación de valores perdidos.

```{r}
imputados.amelia <- Amelia::amelia(datos,m=1,parallel="multicore",noms="C")
incompletos.amelia <- mice::nic(imputados.amelia$imputations$imp1)
completos.amelia <- mice::ncc(imputados.amelia$imputations$imp1)
cat("COMPLETOS = ", completos.amelia, " INCOMPLETOS = ",incompletos.amelia)
datos <- imputados.amelia$imputations$imp1
```

Escalado y centrado

```{r}
datosPrep <- caret::preProcess(datos[,-ncol(datos)],method = c("center","scale"))
datos[,-ncol(datos)] <- predict(datosPrep,datos[,-ncol(datos)])
datosPrep <- caret::preProcess(test,method = c("center","scale"))
test <- predict(datosPrep,test)
```


Eliminacion de ruido

```{r}
set.seed (1)
datos$C <- as.factor(datos$C)
out <- IPF(C~., data = datos ,s = 3)
summary(out, explicit =TRUE)
out$cleanData
datosSinRuido <- datos[setdiff(1:nrow(datos),out$remIdx),]
cat("Se han eliminado ", dim(datos)[1]- dim(datosSinRuido)[1], "instancias")
datos <- datosSinRuido
```

```{r}
corrplot(cor(datos[,-ncol(datos)]))
```


```{r}
datos <- datos[,c("X16","X23","X27","X42","X3","X18","X12","X34","C")]
test <- test[,c("X16","X23","X27","X42","X3","X18","X12","X34")]
```

Proporcion de datos de cada clase:

```{r}
table(datos$C)
```

Como vemos, el problema está bastante desbalanceado. 

```{r}
library(unbalanced)
```

TOMEK LINKS

```{r}
balanced <- ubTomek(datos[,-ncol(datos)],datos$C)
```


```{r}
datos.balanceados <- balanced$X
datos.balanceados$C <- balanced$Y
table(datos.balanceados$C)
```


Generacion del modelo


```{r}
train_pred <- generarModelo(train = datos.balanceados, test = datos.balanceados[,-ncol(datos.balanceados)],k=3)
accuracy<- precision(train_pred,datos.balanceados$C)
accuracy
```

Cálculo de etiquetas del test

```{r}
test_pred <- generarModelo(train = datos.balanceados, test = test,3)
```


```{r}
generarEnvio(y=test_pred,file="envio_tomek_links2.csv")
```


UNDERSAMPLING

```{r}
balanced <- ubUnder(X=datos[,-ncol(datos)], Y= datos$C, perc = 40, method = "percPos")
```

```{r}
datos.undersampling <- balanced$X
datos.undersampling$C <- balanced$Y
table(datos.undersampling$C)
```



Generacion del modelo


```{r}
train_pred <- generarModelo(train = datos.undersampling, test = datos.undersampling[,-ncol(datos.undersampling)],k=3)
accuracy<- precision(train_pred,datos.undersampling$C)
accuracy
```

Cálculo de etiquetas del test

```{r}
test_pred <- generarModelo(train = datos.undersampling, test = test,3)
```


```{r}
generarEnvio(y=test_pred,file="envio_undersampling.csv")
```


OVERSAMPLING

```{r}
balanced <- ubSMOTE(X=datos[,-ncol(datos)], Y= datos$C)
```

```{r}
datos.oversampling <- balanced$X
datos.oversampling$C <- balanced$Y
table(datos.oversampling$C)
```



Generacion del modelo

SOBREAJUSTA MIL


```{r}
train_pred <- generarModelo(train = datos.oversampling, test = datos.oversampling[,-ncol(datos.oversampling)],k=3)
accuracy<- precision(train_pred,datos.oversampling$C)
accuracy
```

Cálculo de etiquetas del test

```{r}
test_pred <- generarModelo(train = datos.oversampling, test = test,3)
```


```{r}
generarEnvio(y=test_pred,file="envio_oversampling.csv")
```



UNDERSAMPLIG CNN + TOMEK LINKS


```{r}
data.cnn <- ubCNN(X = datos[,-ncol(datos)],Y = datos[,ncol(datos)])
data.cnn <- ubTomek(X=data.cnn$X,Y=data.cnn$Y)
```

RESULTADOS DE MÉTODOS PARA DESBALANCEO
```{r}
resultados <- data.frame(modelo = c("tomek_links","undersampling","oversampling"),
                         accuracy.train = c(0.957432,0.9253442,0.9786026),
                         accuracy.test = c(0.86676,0.85604,0.83767))
resultados
```


##MODELO CON SELECCION DE CARACTERISTICAS + PARTICION DE DATOS + SMOTE + OUTLIERS

primero imputacion, despues escalado, particion, ruido


```{r}
datos <- read.csv("../train.csv",na.strings=c(" ","NA","?"))
test <- read.csv("../test.csv",na.strings=c(" ","NA","?"))
```


Imputación de valores perdidos.

```{r}
imputados.amelia <- Amelia::amelia(datos,m=1,parallel="multicore",noms="C")
incompletos.amelia <- mice::nic(imputados.amelia$imputations$imp1)
completos.amelia <- mice::ncc(imputados.amelia$imputations$imp1)
cat("COMPLETOS = ", completos.amelia, " INCOMPLETOS = ",incompletos.amelia)
datos <- imputados.amelia$imputations$imp1
```

Escalado y centrado

```{r}
datosPrep <- caret::preProcess(datos[,-ncol(datos)],method = c("center","scale"))
datos[,-ncol(datos)] <- predict(datosPrep,datos[,-ncol(datos)])
datosPrep <- caret::preProcess(test,method = c("center","scale"))
test <- predict(datosPrep,test)
```


Eliminacion de ruido

```{r,echo=FALSE}
set.seed (1)
datos$C <- as.factor(datos$C)
out <- IPF(C~., data = datos ,s = 3)
summary(out, explicit =TRUE)
out$cleanData
datosSinRuido <- datos[setdiff(1:nrow(datos),out$remIdx),]
cat("Se han eliminado ", dim(datos)[1]- dim(datosSinRuido)[1], "instancias")
datos <- datosSinRuido
```

```{r}
m <- cor(datos[,-ncol(datos)])
correlados <- findCorrelation(m, cutoff = 0.8)
datos <- datos[,-correlados]
test <- test[,-correlados]
```


Desbalanceo 1

```{r}
table(datos$C)
```

```{r}
datos.smote<- oversample(datos,ratio=0.65, method="SMOTE",classAttr = "C")
table(datos.smote$C)
```

Vamos a ver si ha generado ruido.

```{r,echo=FALSE}
set.seed (1)
datos.smote$C <- as.factor(datos.smote$C)
out <- IPF(C~., data = datos.smote ,s = 3)
summary(out, explicit =TRUE)
out$cleanData
datosSinRuido <- datos.smote[setdiff(1:nrow(datos.smote),out$remIdx),]
cat("Se han eliminado ", dim(datos.smote)[1]- dim(datosSinRuido)[1], "instancias")
datos.smote <- datosSinRuido
```

```{r}
table(datos.smote$C)
```

Outliers


```{r}
resOutliers <- mvoutlier::uni.plot(datos.smote[,-ncol(datos.smote)])
```


```{r}
dim(datos.smote)
datos.smote.sinoutliers <- datos.smote[!resOutliers$outliers, ]
dim(datos.smote.sinoutliers)
```


GENERAMOS MODELO

```{r}
train_pred <- generarModelo(train = datos.smote.sinoutliers, test = datos.smote.sinoutliers[,-ncol(datos.smote.sinoutliers)],k=3)
accuracy<- precision(train_pred,datos.smote.sinoutliers$C)
accuracy
```

Cálculo de etiquetas del test

```{r}
test_pred <- generarModelo(train = datos.smote.sinoutliers, test = test,3)
```

Siempre que tenemos un conjunto de datos con muchas variables es conveniente analizarlas desde el punto de vista informativo, es decir, considerar qué variables aportan realmente información a nuestro modelo. Para realizar dicha selección existe muchas medidas y métodos, pero todo depende del objetivo que tengamos.

```{r}
generarEnvio(y=test_pred,file="envio_smote_sinoutliers.csv")
```
