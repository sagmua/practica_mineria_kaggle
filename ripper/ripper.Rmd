---
title: "ripper"
author: "Samuel Cardenete"
date: "4/2/2019"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
#Librerías a usar y funciones útiles:
library(dplyr)
library(Hmisc)
library(mice)
library(Amelia)
library(PerformanceAnalytics)
library(caret)
library(corrplot)
library(VIM)
library(RWeka)
library ( NoiseFiltersR )
library(FSelector)
library(RKEEL)
library(unbalanced)
library(DMwR)



##FUNCIONES AUXILIARES:

source("../funcionesAux.R")
```

## Preprocesamiento 
```{r, include=FALSE}
#lectura de datos:
datos = read.csv("../train.csv", na.string=c(" ", "NA", "?"))
test = read.csv("../test.csv", na.string=c(" ", "NA", "?"))
```

### Valores perdidos y estudio de correlación

Anted de comenzar con los valores perdidos, veamos si existen instancias duplicadas en nuestra base de datos:
```{r}
duplicados = duplicated(datos)
length(which(duplicados))
```
Como vemos tenemos un total de 26 instancias duplicadas, osea, redundantes, asi que las suprimimos.
```{r, include=FALSE}
datos = datos[!duplicados,]
```


```{r, include=FALSE}
anyNA(datos)

#numero de valores perdidos por cada instancia:
numero_na = apply(datos,1, function(x) length(which(is.na(x))))

```
Veamos ahora el número de instancias con más de un valor perdido:
```{r}
nrow(datos[which(numero_na>1),])
```

Como vemos en nuestro conjunto de datos, no tenemos ninguna instancia con más de un valor perdido, pero si que tenemos muchas instancias con un valor perdido, en total:
```{r}
nrow(datos[which(numero_na==1),])

```
Como vemos se trata de un número considerable de valores perdidos. En resumen tenemos que el 15\% de nuestro conjunto de datos de entrenamiento posee valores perdidos, pero como todos sabemos, una imagen vale más que mil palabras, asi que realicemos un análisis gráfico de los valores perdidos:

```{r, echo=FALSE}
plot <- VIM::aggr(datos,col=c('blue','red'),numbers=TRUE,sortVars=TRUE,labels=names(data),
                  cex.axis=.5,gap=1,ylab=c("Gráfico de datos perdidos","Patron"))
```
Como vemos en el gráfico de la izquierda, en la distribución de valores perdidos en las variables, vemos que se encuentran entre el 0.5\% y el 0.02\% en exactamente la mitad de características de nuestro conjunto de datos de entrenamiento. La distribución de los valores perdidos parece ser aleatoria si vemos las densidades de estos.

Además, si observamos la segunda gráfica, podemos ver la independencia entre la aparición de valores perdidos entre variables, de forma que la aparición de un valor perdido en una variable, no implica la aparición de valores perdidos en otros.


Esto requiere sin duda un análisis más exahustivo de los valores perdidos así como posible imputación. Para ello emplearemos el algoritmo de imputación *MICE*. Comenzamos obteniendo el patrón de aparición de los datos perdidos, pero nuevamente obtenemos, al haber solo valores perdidos en una única variable obtenemos que no hay dependencia/patrones de aparición de los valores perdidos.

```{r, include=FALSE}
patron = mice::md.pattern(x = datos)
```
Determinemos las instancias conpletas e incompletas y visualicemoslo:
```{r, echo = FALSE}
completas = mice::ncc(datos)
incompletas = mice::nic(datos)

df <- data.frame(
  group = c("Inompletas", "Completas"),
  value = c(incompletas, completas)
  )
bar <- ggplot(df, aes( x = "", y = value, fill = group)) + geom_bar(width = 1, stat = "identity") 
pie <- bar + coord_polar("y", start=0) + geom_text(aes(x=1, y = cumsum(value) - value*0.5, label=round((value/nrow(datos)*100),digits = 2)))
pie
```

En una primera aproximación, he decidido realizar la imputación de valores perdidos mediante Predictive Mean Matching (*PMM*). La concordancia media predictiva es un enfoque de imputación semiparamétrica. Es similar al método de regresión, excepto que para cada valor perdido, se reemplaza con un valor aleatorio de entre los valores de un donante observado de una observación cuyos valores pronosticados de regresión son los más cercanos al valor predicho de regresión para el valor perdido de la simulación del modelo de regresión.

De esta forma utilizamos una aproximación del valor perdido mucho más precisa que simplemente la media o mediana.
```{r, results="hide", echo=TRUE, warning=FALSE}
#imputados = mice::mice(datos, m=5, meth="pmm")
```

Aquellas variables donde se han realizado imputaciones mediante *PMM*:
```{r}
#imputados$method[imputados$method == "pmm"]
```
Si observamos, MICE no ha realizado la imputación sobre todas las variables de nuestro conjunto de datos eso implica que aún tenemos valores perdidos en los imputados. Pero si observamos el log vemos que ha ignorado las variables colineares, por tanto, podremos quedarnos con el conjunto de variables restantes ya imputadas, de forma que además de realizar la imputación, hemos hecho una selección de variables eliminando las redundantes (altamente colineares):

```{r}
#IMPUTACION CON MICE_
#var_rest = names(imputados$method[imputados$method == "pmm"])
#datos = complete( imputados)
#datos = datos[,append(var_rest, "C")]
#summary(datos)

#IMPUTACION CON AMELIA:
imputados.amelia <- Amelia::amelia(datos,m=1,parallel="multicore",noms="C")
incompletos.amelia <- mice::nic(imputados.amelia$imputations$imp1)
completos.amelia <- mice::ncc(imputados.amelia$imputations$imp1)
cat("COMPLETOS = ", completos.amelia, " INCOMPLETOS = ",incompletos.amelia)
datos <- imputados.amelia$imputations$imp1
```

Hemos reducido nuestro conjunto de datos a 11 variables predictoras, eliminando aquellas altamente correladas.

## Seleccion de características:

Ya llegados a este punto, que hemos realizado la imputación de las variables con MICE y hemos reducido dimendsionalidad, hagamos un estudio de la correlación de las variables que tenemos ahora mismo en nuestro conjunto de datos. 

Para ello vamos primero a emplear la correlación para mediante el paquete FSelector eliminar aquellas variables altamente correladas:


Para ello realizamos una representación gráfica de las correlaciones entre las variables, de forma que a mayor amplitud del círculo mayor correlación existe, los colores fríos indican correlación directa y los cálidos inversa:


```{r}
datos_sin_seleccion = datos
# se encuentran aquellas variables que presentan valores de correlacion
# por encima del valor umbral
highlyCorrelated = caret :: findCorrelation(cor(datos) , cutoff =0.999999)
datos = datos[,-highlyCorrelated]
corrplot(cor(datos))
```
Como vemos, tal como decíamos antes, no tenemos variables directamente altamente correladas, puesto que MICE hace ese filtrado durante el proceso de cálculo de las imputaciones. Pero por el otro lado, si vemos que tenemos muchas variables altamente inversamente correladas, es el caso por ejemplo de *X25* con *X44*, o con *X18*.



```{r}
#Calculamos los pesos mediante la correlación linear con nuestra
# variable clase 'C'
weights = FSelector::linear.correlation(C~., datos)
barplot(weights$attr_importance, names.arg = rownames(weights), las=2)

```
Como podemos observar, la variable *X16*, *X23*, *X27* y *X42* son las variables que más correlación tienen respecto a la variable salida. Esto quiere decir que a priori explican mejor que el resto a esta, por lo que son considerados como buenos predictores para nuestro futuro modelo a generar.

Seleccionemos ahora los 4 mejores atributos en función de los pesos:
```{r}
subset = cutoff.k(weights ,4)
f = as.simple.formula(subset ,"C")
weights = rank.correlation(C~.,datos)
barplot(weights$attr_importance, names.arg = rownames(weights), las=2)

subset = FSelector :: cutoff.k(weights ,9)
f = as.simple.formula(subset ,"C")
print(f)
```


escalado:

```{r}
#datos = datos[,!names(datos) %in% c("X25", "X44")]
#datos [,-which(names(datos) == "C")] = scale(datos[,- which(names(datos) == "C")], center = TRUE, scale = TRUE)
datos$C = as.factor(datos$C)
datos_sin_seleccion$C = as.factor(datos_sin_seleccion$C)
```

Eliminación de ruido:
```{r, include=FALSE}
set.seed (1)
out = IPF(C~., data = datos , nfolds=5, s = 3)
summary(out, explicit =TRUE)
identical(out$cleanData , datos[ setdiff (1:nrow(datos) ,out$remIdx) ,])
datos = out$cleanData

datos_sin_seleccion_out = IPF(C~., data = datos_sin_seleccion , nfolds=5, s = 3)
datos_sin_seleccion = datos_sin_seleccion_out$cleanData

```

## Datos anómalos:
```{r}
anomalos <- outliers::outlier(datos[,-ncol(datos)])
print(anomalos)


resOutliers1 <- mvoutlier::uni.plot(datos[1:10])
#resOutliers2 <- mvoutlier::uni.plot(datos[11:ncol(datos)])
#resOutliers3 <- mvoutlier::uni.plot(datos[21:30])
#resOutliers4 <- mvoutlier::uni.plot(datos[31:40])
#resOutliers2 <- mvoutlier::uni.plot(datos[c(41:44,46:50)])




```

## Desbalanceo
Como hemos visto durante este análisis previo tenemos un conjunto de datos desbalanceado, donde tenemos más instancias de clase '0' que de clase '1'.

Para paliar este problema procedemos a utilizar el algoritmo SMOTE:

```{r}
#datos_b = SMOTE(C~., data = datos_sin_seleccion, perc.over = 200, k=5, perc.under = 150)

```

Probemos ahora con rmek:

```{r}

set.seed(3)
salidaTomek = ubTomek(datos_duplicados[,-ncol(datos_duplicados)], datos_duplicados$C)

datosTomek = cbind(salidaTomek$X, C = salidaTomek$Y)

dim(datosTomek)

proporcion = group_by(datosTomek,C) %>% summarise(propo = round((nc = (n() * 100)/dim(datosTomek)[1]),digits = 1))

proporcion

df <- data.frame(clase=proporcion$C,
                proporcion=proporcion$propo)

p<-ggplot(df, aes(x=clase, y=proporcion, fill=clase)) +
  geom_bar(stat="identity")+theme_minimal()
p


```
```{r}
datos_b = SMOTE(C~., data = datosTomek, perc.over = 150, k=5, perc.under = 0)
datos_b = rbind(subset(datos_b, C==1), subset(datosTomek, C==0))

```





## Modelos RIPPER

Llegados a este punto, donde el preprocesamiento está realizado, sólo nos queda obtener el mejor módelo basado en reglas de asociación mediante el algoritmo RIPPER que mejor se adapte a nuestro conjunto de datos:

```{r}
#test = as.data.frame(scale(test, center = TRUE, scale = TRUE))
#discretizar:
#datos = discretization::disc.Topdown(datos, method=1)

modelRipp <- JRip(formula = C~.,data = datosTomek)
#prediction = predict(modelRipp, datos)
```

Calculamos la precisión para el conjunto train:
```{r}
#precision(predictionLabels = prediction, testLabels = datos$C)
evaluate_Weka_classifier(modelRipp, numFolds = 5, seed = 3)
```
Generamos un primer envío, sin discretizar ni normalizar, ni outliers:
```{r}
prediction = predict(modelRipp, test)
generarEnvio(prediction, file = "datos_tomek_con_seleccion:solo_una.csv")

```





