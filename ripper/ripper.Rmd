---
title: "ripper"
author: "Samuel Cardenete"
date: "4/2/2019"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
#Librerías a usar y funciones útiles:
library(dplyr)
library(Hmisc)
library(mice)
library(Amelia)
library(PerformanceAnalytics)
library(caret)
library(corrplot)
library(VIM)
library(RWeka)
library ( NoiseFiltersR )


##FUNCIONES AUXILIARES:

source("../funcionesAux.R")
```

## Preprocesamiento 
```{r, include=FALSE}
#lectura de datos:
datos = read.csv("../train.csv", na.string=c(" ", "NA", "?"))
test = read.csv("../test.csv", na.string=c(" ", "NA", "?"))
```

### Valores perdidos y estudio de correlación
```{r, include=FALSE}
anyNA(datos)

#numero de valores perdidos por cada instancia:
numero_na = apply(datos,1, function(x) length(which(is.na(x))))

```
Número de instancias con más de un valor perdido:
```{r}
nrow(datos[which(numero_na>1),])
```

Como vemos en nuestro conjunto de datos, no tenemos ninguna instancia con más de un valor perdido, pero si que tenemos muchas instancias con un valor perdido, en total:
```{r}
nrow(datos[which(numero_na==1),])

```
Como vemos se trata de un número considerable de valores perdidos. En resumen tenemos que el 15\% de nuestro conjunto de datos de entrenamiento posee valores perdidos, pero como todos sabemos, una imagen vale más que mil palabras, asi que realicemos un análisis gráfico de los valores perdidos:

```{r, echo=FALSE}
plot <- VIM::aggr(datos,col=c('blue','red'),numbers=TRUE,sortVars=TRUE,labels=names(data),
                  cex.axis=.5,gap=1,ylab=c("Gráfico de datos perdidos","Patron"))
```
Como vemos en el gráfico de la izquierda, en la distribución de valores perdidos en las variables, vemos que se encuentran entre el 0.5\% y el 0.02\% en exactamente la mitad de características de nuestro conjunto de datos de entrenamiento. La distribución de los valores perdidos parece ser aleatoria si vemos las densidades de estos.

Además, si observamos la segunda gráfica, podemos ver la independencia entre la aparición de valores perdidos entre variables, de forma que la aparición de un valor perdido en una variable, no implica la aparición de valores perdidos en otros.


Esto requiere sin duda un análisis más exahustivo de los valores perdidos así como posible imputación. Para ello emplearemos el algoritmo de imputación *MICE*. Comenzamos obteniendo el patrón de aparición de los datos perdidos, pero nuevamente obtenemos, al haber solo valores perdidos en una única variable obtenemos que no hay dependencia/patrones de aparición de los valores perdidos.

```{r, include=FALSE}
patron = mice::md.pattern(x = datos)
```
Determinemos las instancias conpletas e incompletas y visualicemoslo:
```{r, echo = FALSE}
completas = mice::ncc(datos)
incompletas = mice::nic(datos)

df <- data.frame(
  group = c("Inompletas", "Completas"),
  value = c(incompletas, completas)
  )
bar <- ggplot(df, aes( x = "", y = value, fill = group)) + geom_bar(width = 1, stat = "identity") 
pie <- bar + coord_polar("y", start=0) + geom_text(aes(x=1, y = cumsum(value) - value*0.5, label=round((value/nrow(datos)*100),digits = 2)))
pie
```

En una primera aproximación, he decidido realizar la imputación de valores perdidos mediante Predictive Mean Matching (*PMM*). La concordancia media predictiva es un enfoque de imputación semiparamétrica. Es similar al método de regresión, excepto que para cada valor perdido, se reemplaza con un valor aleatorio de entre los valores de un donante observado de una observación cuyos valores pronosticados de regresión son los más cercanos al valor predicho de regresión para el valor perdido de la simulación del modelo de regresión.

De esta forma utilizamos una aproximación del valor perdido mucho más precisa que simplemente la media o mediana.
```{r, results="hide", echo=TRUE, warning=FALSE}
imputados = mice::mice(datos, m=5, meth="pmm")
```

Aquellas variables donde se han realizado imputaciones mediante *PMM*:
```{r}
imputados$method[imputados$method == "pmm"]
```
Si observamos, MICE no ha realizado la imputación sobre todas las variables de nuestro conjunto de datos eso implica que aún tenemos valores perdidos en los imputados. Pero si observamos el log vemos que ha ignorado las variables colineares, por tanto, podremos quedarnos con el conjunto de variables restantes ya imputadas, de forma que además de realizar la imputación, hemos hecho una selección de variables eliminando las redundantes (altamente colineares):

```{r}
var_rest = names(imputados$method[imputados$method == "pmm"])
datos = complete( imputados)
datos = datos[,append(var_rest, "C")]
summary(datos)
```

Hemos reducido nuestro conjunto de datos a 11 variables predictoras, eliminando aquellas altamente correladas.

Ya llegados a este punto, que hemos realizado la imputación de las variables ocn MICE y hemos reducido dimendsionalidad, hagamos un estudio de la correlación de las variables que tenemos ahora mismo en nuestro conjunto de datos. 
Para ello realizamos una representación gráfica de las correlaciones entre las variables, de forma que a mayor amplitud del círculo mayor correlación existe, los colores fríos indican correlación directa y los cálidos inversa:


```{r}
corrplot(cor(datos))
```
Como vemos, tal como decíamos antes, no tenemos variables directamente altamente correladas, puesto que MICE hace ese filtrado durante el proceso de cálculo de las imputaciones. Pero por el otro lado, si vemos que tenemos muchas variables altamente inversamente correladas, es el caso por ejemplo de *X25* con *X44*, o con *X18*.

Por tanto esto nos indica que existe redundancia, puesto que las variables altamente correladas se explican entre sí. Por lo que podemos reducir estas mismas eliminando *X25* y *X44*:

```{r}
#datos = datos[,!names(datos) %in% c("X25", "X44")]
datos$C = as.factor(datos$C)
```

Eliminación de ruido:
```{r, echo=FALSE}
set.seed (1)
out = IPF(C~., data = datos , nfolds=5, s = 3)
summary(out, explicit =TRUE)
identical(out$cleanData , datos[ setdiff (1:nrow(datos) ,out$remIdx) ,])

```


## Modelos RIPPER

Llegados a este punto, donde el preprocesamiento está realizado, sólo nos queda obtener el mejor módelo basado en reglas de asociación mediante el algoritmo RIPPER que mejor se adapte a nuestro conjunto de datos:

```{r}
modelRipp <- JRip(C~.,data = out$cleanData)
prediction = predict(modelRipp, datos[- out$remIdx,- which(names(datos) == "C")])
```

Calculamos la precisión para el conjunto train:
```{r}
precision(predictionLabels = prediction, testLabels = datos$C[-out$remIdx])
```
Generamos un primer envío, sin discretizar ni normalizar, ni outliers:
```{r}
prediction = predict(modelRipp, test)
generarEnvio(prediction, file = "con_todos_sin_ruido.csv")
```





