---
title: "Decision Trees"
author: "Juan Ramón Gómez Berzosa"
date: "4/2/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}

#Librerías a usar:
library(dplyr)
library(Hmisc)
library(mice)
library(ggplot2)
library(caret)
library(lattice)
library(VIM)
library(Amelia)
library(partykit)
library(RWeka)
library(tree)
require(robCompositions)
library(NoiseFiltersR)
library(outliers)
require(mvoutlier)
library(mlbench)
library(corrplot)
library(FSelector)
library(unbalanced)
library(ROSE)
library(Boruta)

#Funciones auxiliares:
setwd("~/GitHub/practica_mineria_kaggle")
source("funcionesAux.R")
```
#Árboles de clasificación

En este apartado realizaremos el preprocesamiento del conjunto de datos necesario para poder aprender el mejor clasificador basado en árboles, el cual obtenga la mayor precisión posible en la clasificación de nuestro conjunto de datos de prueba. 

## Preprocesamiento 
En primer lugar haremos la lectura de los datos.
```{r}
#lectura de datos:
setwd("~/GitHub/practica_mineria_kaggle")
train = read.csv("train.csv", na.string=c(" ", "NA", "?"))

test = read.csv("test.csv", na.string=c(" ", "NA", "?"))
#Pasamos a formato TIBBLE:
train = as_tibble(train)
train$C = as.factor(train$C)
test = as_tibble(test)
dim(train)
dim(test)
```

Nuestro dataset consta 13063 instancias para aprender nuestro clasificador, 9144 para train y 3919 para test, teniendo un total de 50 caraterísticas para cada observación y una variable de clase. Como es lógico como estamos realizando una competición, de la parte del test no tenemos información de clase ya que se comprobará el porcentaje de acierto una vez subamos nuestra clasificación del test a kaggle. También cabe destacar que hay un conjunto de datos que se encuentran en privado en kaggle, los cuales se usarán para ver los ganadores de la competición y sólo se desbloqueará una vez se cierre el plazo.

En primer lugar analizaremos el conjunto de datos de entrenamiento con el objetivo de observar las tendencias de los datos, así como la existencia y tratamiento de valores perdidos, outliers y ruido. 


###Análisis inicial

Comenzaremos echando un vistazo al summary para obtener una información inicial.

```{r summary}
#describe(train
summary(train)
#ggplot(data=train) + geom_bar(aes(x=C, y = ..prop.., group=1))
```
En primer lugar cabe destacar que todas las variables de nuestro conjunto de datos de entrenamiento son reales, a excepción de la variable de clase. Con un vistazo inicial podemos apreciar la presencia de valores perdidos en cada atributo, sin embargo observamos que la cantidad de missing values por cada característica es bastante similar, de modo que el número varía entre 17 y 35, a excepción de dos variables con 41 y 50. Esto nos puede llevar a dar una pista de que seguramente los missing values no estén por casualidad y hayan sido distribuidos de forma aleatoria en este rango por cada una de las variables, de forma intencionada. Por tanto, estaremos dentro del caso de "MCAR: missing values completely at random".

Para comenzar, debido a que tenemos tal cantidad de observaciones vamos a observar si existen observaciones repetidas y procederemos a eliminarlas.

```{r unique}
train = unique(train)
dim(train)
#summary(train)
```

Como hemos observado teníamos algunas observaciones repetidas, así que las hemos eliminado y nos hemos quedado con un total de 26 observaciones. Como es normal, los valores en cuanto a missing values no habrán cambiado debido a que estas instancias con valores perdidos no se podían comparar con el resto.

Ahora vamos a observar la proporción de la variable de clase con respecto a su presencia en las distintas observaciones para comprobar si existe desbalanceo en el problema.

```{r proporcionClase}

proporcion = group_by(train,C) %>% summarise(propo = round((nc = (n() * 100)/dim(train)[1]),digits = 1))
proporcion

df <- data.frame(clase=proporcion$C,
                proporcion=proporcion$propo)

p<-ggplot(df, aes(x=clase, y=proporcion, fill=clase)) +
  geom_bar(stat="identity")+theme_minimal()
p

```

Podemos observar como el 65.5% de las observaciones corresponden a la clase 0 y el 35.5% restante a la clase 1, por lo tanto tenemos un conjunto de datos no balanceado.

### Missing Values

Antes de empezar con el entrenamiento del clasificador, es importante tratar los valores perdidos del conjunto de datos. Cabe destacar que aunque vayamos a tratar con clasificadores basados en árboles, tenemos que lidiar con los missing values ya que al no permitirse en al competición utilizar multiclasificadores basados en árboles estos si son sensibles a la existencia de valores perdidos. Afectan de tres formas principalmente:

  - Afecta en como se calculan las medidas de impureza.
  - Afecta en como distribuir las instancias con missing values a los nodos hijos.
  - Afecta en como una instancia del test con missing values es clasificada.

En primer lugar vamos a estudiar el último caso, ya que este nos puede influir mucho a la hora de preprocesar los datos. Para ello comprobaremos si existen valores perdidos en el test.

```{r }

which(is.na(test))

```
Como podemos ver no tenemos atributos con valores perdidos, lo cual nos favorece ya que además de no darnos problemas con el algoritmo de clasificación no nos obliga a tener que realizar las mismas transformaciones en el test que en el train a la hora del preprocesamiento. Por tanto, el último caso no nos interesa mucho pero si que nos afectan en las dos restantes.

Vamos a ver el número de instancias completas e incompletas y además observaremos si existe un patrón seguido por los valores perdidos, viendo su distribución por las instancias.

```{r patron missing values}
patron <- mice::md.pattern(x=train[,-51], plot=TRUE)

completas <- mice::ncc(train)
incompletas <- mice::nic(train)


completas
incompletas
```


```{r other function nas, include = false }


# Referencias a los paquetes necesarios para disponer
# del conjunto de datos y de la funcion de visualizacion # del patron de datos perdidos
require(VIM)
#Se visualiza un grafico que nos indica la forma en que se distribuyen los # datos perdidos
datos = airquality
# se genera el grafico de distribucion de datos perdidos . Solo 
# se consideran las variables con datos perdidos
plot = aggr(train , col=c("blue","red") , numbers=TRUE, sortVars=TRUE,labels=names(train), cex.axis=.5, gap=1, ylab=c("Grafico de datos perdidos","Patron"))


```
```{r Amelia}
cat("Datos completos: ",completas, " e incompletos: ",incompletas,"\n")
missmap(train[1:300,1:20], col= c("red","steelBlue"))

```

Tenemos un total de 7752 instancias completas y 1366 instancias a las que les falta al menos una característica, aproximadamente el 15% de las observaciones. En todas las variables hay presencia de missing values como hemos comentado antes (menos en la variable de clase, como es lógico), pero lo que es llamativo es que siguen una distribución peculiar. En todas las observaciones que tienen valores perdidos, sólo hay presencia de missing values en una característica. Además, también es llamativo el número de observaciones con missing values para cada atributo, ya que como hemos comentado antes parece que se han distribuido los valores perdidos de forma intencionada de modo que se han elegido un número de instancias aleatorias entre 15 y 50 (el número de variables) en las que se han colocado un missing value en alguna de sus características. Cabe destacar que en el test, como hemos comentado antes, no tienen missing values y todas sus observaciones son completas.

Debido a que aproximadamente tenemos un 10% del conjunto de observaciones con valores perdidos y que estos están distribuidos por todas las características, no debemos de eliminar ni dichas observaciones ni dichos atributos porque supondría una enorme perdida de información. Por tanto, debemos de llevar a cabo algunas técnicas de imputación para poder lidiar con estos. 

En primer lugar empezaremos aplicando métodos de imputación del paquete mice, en concreto partiremos del método cart el cual se corresponde con una imputación por medio de árboles. Conviene utilizar algún método de este tipo o más complejo debido a que si sustiuimos por la media o la desviación no lleva a unos resultados precisos. Estos métodos más complejos de imputación consiste en generar conjuntos de datos aleatorios en diferentes particiones (las cuales indicamos con el parametro m de mice, batch) de forma que las imputaciones se van realizando sobre ellas. De esta forma, cada imputación realizada sobre una partición se utiliza para la siguiente, así se obtienen resultados cada vez más precisos.

```{r imputacion mice}
#methods(mice)
# se realiza la imputacion
# 5 batch (series) diferentes de imputaciones. El método pmm es el de imputación que se utiliza
imputados <- mice::mice(train, m=5, meth="cart", parallel = "multicore")

# se completa el conjunto de datos con las imputaciones
datosImputados <- mice::complete(imputados)


# se determina el numero de instancias sin datos perdidos y con datos
# perdidos en la parte ya limpia
completos <- mice::ncc(datosImputados)
incompletos <- mice::nic(datosImputados)
cat("Datos completos: ",completos, " e incompletos: ",incompletos,"\n")



```

El algoritmo mice está diseñado para descartar las variables correladas, por lo que un punto fuerte de este método es que al no tener en cuenta las variables correladas este prescinde de ellas automáticamente por lo que se reduce la dimensionalidad del conjunto de datos. 

Sin embargo, aunque hemos aplicado mice no se completan todas las observaciones debido a lo que hemos comentadop anteriormente. Por tanto, procederemos a utilizar el paquete Amelia, el cual no ignora estas variables y por tanto puede funcionar mejor. De todas formas, posteriormente haremos un estudio exhausto de correlación para comprobar que ocurre realmente.

```{r imputados amelia}


imputados.amelia <- Amelia::amelia(train,m=5,parallel="multicore",noms="C")
incompletos.amelia <- mice::nic(imputados.amelia$imputations$imp5)
completos.amelia <- mice::ncc(imputados.amelia$imputations$imp5)
cat("COMPLETOS = ", completos.amelia, " INCOMPLETOS = ",incompletos.amelia)
datosImputadosAmelia <- imputados.amelia$imputations$imp5
```


Como podemos observar, el resultado de esta imputación si que nos imputa todos los valores perdidos y por tanto ya tenemos todas las instancias del conjunto de datos completas.

Ahora aplicaremos el paquete RobCompositions, el cual realiza imputación utilizando Knn, para intentar mejorar la imputación realizada anteriormente.


```{r imputacion RobCompositions}


# se hace la imputacion
imputados <- robCompositions::impKNNa(train, primitive=TRUE)

# Ahora puede visualizarse alguna informacion sobre la forma
# en que se hizo la imputacion. El segundo argumento indica el
# tipo de grafico a obtener
plot(imputados, which=2)

# El conjunto de datos completo puede accederse de la siguiente forma
datosImputadosRobCompositions =  imputados$xImp

# se determina el numero de instancias sin datos perdidos y con datos
# perdidos. A observar la comodidad de uso de las funciones ncc e nic
completos <- mice::ncc(datosImputadosRobCompositions)
incompletos <- mice::nic(datosImputadosRobCompositions)
cat("Datos completos: ",completos, " e incompletos: ",incompletos,"\n")

```
Como podemos observar, con este paquete también obtenemos el conjunto de datos con todos los valores imputados. Según el gráfico casi todas las variables imputadas siguen un patron similar, excepto en algunos casos.

Con esto terminaríamos la imputación de los datos a priori, a la hora de decidir con cual nos quedaremos de las 3 imputaciones, en cuanto a resultados podríamos quedarnos con Amelia o RobCompositions. ---- LUEGO DECIDIMOS----

### Detección de Ruido

Una parte esencial del preprocesamiento de los datos es la detección de ruido. Algunos métodos son especialmente vulnerables al ruido, entre ellos se encuentran los árboles de decisión. El ruido básicamente se trata de la presenecia de valores que son distintos de los esperados en algunas variables de las instancias del conjunto de datos. Por tanto, el paso que haremos a continuación será analizar nuestro conjunto de datos de entrenamiento con el fin de detectar posible ruido y por tanto intentar suavizarlo. 

Para ello usaremos uno de los algoritmos más eficaces en este campo, Iterative Partitioning Filter. Este algoritmo se centra en el ruido de clase, por lo que su función es eliminar instancias mal etiquetas. Realiza varias iteracciones hasta alcanzar una condición de parada que suele ser que el número de instancias consideradas como ruido esté por debajo de cierto umbral. 

```{r ruido}

# se inicializa la semilla aleatoria para reproducir los resultados
set.seed(1)

datos = as.data.frame(datosImputadosAmelia)

datos$C = as.factor(datos$C)
# se aplica el algoritmo 
# El s = 2 establece el criterio de parada el cual indica que el algoritmo para
# después de encontrarse sin eliminar instancias tras dos iteraciones
out <- IPF(C~., data = datos, s = 2)

# se muestran los indices de las instancias con ruido
#summary(out, explicit = TRUE)

# el conjunto de datos sin ruido se obtiene de la siguiente forma
out$cleanData

# tambien podriamos obtenerlo de forma directa eliminando los
# indices de las instancias consideradas como ruidosas
datosSinRuido <- datos[setdiff(1:nrow(datos),out$remIdx),]

datosSinRuido = as.data.frame(datosSinRuido)
str(datosSinRuido)


```


Como podemos observar, se han detectado 1572 instancias con ruido y por tanto las hemos eliminado, aproximadamente un 17% del conjunto de datos era ruido. Atendiendo a esto, podemos afirmar que el ruido ha sido introducido de forma intencionada en el dataset siguiendo alguno de los métodos de introducción de ruido existentes.

Una vez comprobado el ruido, vamos a volver a comprobar la distribución de clases del conjunto ya que seguramente debido a aplicar esta técnica el conjunto se haya desbalanceado más aún.

```{r proporcionClaseDespuesIPF}

proporcion = group_by(datosSinRuido,C) %>% summarise(propo = round((nc = (n() * 100)/dim(datosSinRuido)[1]),digits = 1))
proporcion

df <- data.frame(clase=proporcion$C,
                proporcion=proporcion$propo)

p<-ggplot(df, aes(x=clase, y=proporcion, fill=clase)) +
  geom_bar(stat="identity")+theme_minimal()
p

```
Como hemos podido adelantar, al aplicar IPF hemos provocado un desbalanceo aún más grande, por tanto ahora tenemos un 73.1% de la clase 0 y un 26.9 de la clase 1, con respecto a los valores iniciales de 65.5% y 34.5% respectivamente. Debido a este desbalanceo podemos plantearnos utilizar técnicas de oversampling y undersampling para intentar balancear más el conjunto de datos y por tanto obtener mejores resultados.



---- RUIDO SIN IPF --- 

```{r ruido}

# se inicializa la semilla aleatoria para reproducir los resultados
set.seed(1)

datos = as.data.frame(datosImputadosAmelia)

datos$C = as.factor(datos$C)
# se aplica el algoritmo 
out = EF(C~., data = datos, consensus = FALSE)

# se muestran los indices de las instancias con ruido
#summary(out, explicit = TRUE)

# el conjunto de datos sin ruido se obtiene de la siguiente forma
out$cleanData

# tambien podriamos obtenerlo de forma directa eliminando los
# indices de las instancias consideradas como ruidosas
datosSinRuidoEF <- datos[setdiff(1:nrow(datos),out$remIdx),]

datosSinRuidoEF = as.data.frame(datosSinRuidoEF)
str(datosSinRuidoEF)


```

```{r ruido}

# se inicializa la semilla aleatoria para reproducir los resultados
set.seed(1)

datos = as.data.frame(datosImputadosAmelia)

datos$C = as.factor(datos$C)
# se aplica el algoritmo 
out = CVCF(C~., data = datos, consensus = FALSE)

# se muestran los indices de las instancias con ruido
#summary(out, explicit = TRUE)

# el conjunto de datos sin ruido se obtiene de la siguiente forma
out$cleanData

# tambien podriamos obtenerlo de forma directa eliminando los
# indices de las instancias consideradas como ruidosas
datosSinRuidoCVCF <- datos[setdiff(1:nrow(datos),out$remIdx),]

datosSinRuidoCVCF = as.data.frame(datosSinRuidoCVCF)
str(datosSinRuidoCVCF)


```
### Anomalías

Después de comparar los resultados en kaggle, hemos conseguido mejorar la precisión de nuestro clasificador al suavizar el ruido del dataset de entrenamiento. Sin embargo, en los conjuntos de datos pueden existir otro tipo de valores erróneos que no se pueden considerar ruido. Estos valores se corresponden con los de aquellas instancias cuyas características son muy diferentes con respecto a la mayoría de los demás datos, por tanto se denominan outliers o valores extremos. En ocasiones, su presencia es producida por errores en la recogida de los datos 

En general, los algoritmos de árboles de clasificación no son muy sensibles a outliers, ya que al poder tratar con variables continuas establecen particiones en los dominios teniendo en cuenta valores límite (outliers). Sin embargo, analizaremos la presencia de outliers en el conjunto de datos.

```{r anomaliasboxplot}

boxplot(datosSinRuido[,-51])


```

Como podemos observar, tenemos el mismo valor outlier en cada variable. Esto seguramente se deba a que ha sido introducido de forma intencionada en cada variable. Lo que más se puede ver a simple vista es que las variables X3 y X16 tienen muchos outliers.


### Transformación de los datos

Este proceso está fundamentalmente orientado para aquellos algoritmos que utilizan medidas de distancia para generar el método de aprendizaje. Sin embargo, en árboles de clasificación no es necesario realizar esta etapa del preprocesamiento ya que en ningún momento se utilizan métricas de distancia. En cambio, en el caso de árboles de regresión si que sería útil ya que si que utilizan métricas de distancia.

### Discretización

Algunos técnicas de apredizaje precisan de una discretización previa de las variables continuas debido a que no son capaces de trabajar con variables de este tipo. Sin embargo, los árboles trabajan bien con variables de este tipo.


### Balanceo del dataset

Como hemos comentado anteriormente, el dataset después de aplicar la función de reducción de ruido se ha desbalanceado aún más. Por tanto, vamos a intentar aplicar técnicas de oversampling y oversampling para intentar que mejore la precisión de nuestro modelo.

#### Oversampling

##### SMOTE

```{r smote}


salidaSmote = ubSMOTE(datosSinRuido[,-51],datosSinRuido$C)

datosSmote = cbind(salidaSmote$X, C = salidaSmote$Y)

dim(datosSmote)
```

Podemos ver como hemos introducido el doble de datos de las que teníamos. Vamos a comprobar el desbalanceo.

```{r proporcionClaseDespuesSMOTE}

proporcion = group_by(datosSmote,C) %>% summarise(propo = round((nc = (n() * 100)/dim(datosSmote)[1]),digits = 1))
proporcion

df <- data.frame(clase=proporcion$C,
                proporcion=proporcion$propo)

p<-ggplot(df, aes(x=clase, y=proporcion, fill=clase)) +
  geom_bar(stat="identity")+theme_minimal()
p

```

Podemos observar como ya existe más equitatividad en cuanto a clases. Por tanto, veremos si mejoran los modelos. NO MEJORAN LOS MODELOS

ROS
```{r ros}

set.sed(1)
data.balanced.ou <- ovun.sample(C~., data=datosSinRuido, p=0.5, seed=1, method="over")$data

#data.balanced.ou
dim(data.balanced.ou)

proporcion = group_by(data.balanced.ou,C) %>% summarise(propo = round((nc = (n() * 100)/dim(data.balanced.ou)[1]),digits = 1))
proporcion

df <- data.frame(clase=proporcion$C,
                proporcion=proporcion$propo)

p<-ggplot(df, aes(x=clase, y=proporcion, fill=clase)) +
  geom_bar(stat="identity")+theme_minimal()
p
```

#### Undersampling

#### Tomek Link
```{r tomeklink}
set.seed(2)
salidaTomek = ubTomek(datosSinRuido[,-51], datosSinRuido$C)

datosTomek = cbind(salidaTomek$X, C = salidaTomek$Y)

dim(datosTomek)

proporcion = group_by(datosTomek,C) %>% summarise(propo = round((nc = (n() * 100)/dim(datosTomek)[1]),digits = 1))
proporcion

df <- data.frame(clase=proporcion$C,
                proporcion=proporcion$propo)

p<-ggplot(df, aes(x=clase, y=proporcion, fill=clase)) +
  geom_bar(stat="identity")+theme_minimal()
p
```

```{r tomeklink}
set.seed(2)

salidaTomek = ubTomek(datosTomek[,-51], datosTomek$C)

datosTomek = cbind(salidaTomek$X, C = salidaTomek$Y)

dim(datosTomek)

proporcion = group_by(datosTomek,C) %>% summarise(propo = round((nc = (n() * 100)/dim(datosTomek)[1]),digits = 1))
proporcion

proporcion1 = group_by(datosSinRuido,C) %>% summarise(propo = (nc = (n())))
proporcion1
proporcion1 = group_by(datosTomek,C) %>% summarise(propo = (nc = (n())))
proporcion1


df <- data.frame(clase=proporcion$C,
                proporcion=proporcion$propo)

p<-ggplot(df, aes(x=clase, y=proporcion, fill=clase)) +
  geom_bar(stat="identity")+theme_minimal()
p
```
Con dos iteraciones mejora el model en kaggle y en el train: 
Instances removed 206 : 4.02 % of 0 class ; 2.88 % of training ; Time needed 0.05 
[1] 6943   51

Con tres iteraciones: no mejora. se queda en 0.911

Instances removed 140 : 2.85 % of 0 class ; 2.02 % of training ; Time needed 0.05 
[1] 6803   51



Después de probar over and under no hemos encontrado mejoras signficativas en el modelo por ahora. Ahora pasaremos a la selección de características.

### Selección de características: Filter

En primer lugar probaremos con la estrategia de filtrado, la cual se basa en calcular la importancia de las variables para el modelo en función de una medida estadística. De los paquetes y los tipos que nos han sido asignados aplicaremos los distintos tipos y compararemos cual nos da mejor resultado en términos de precisión e interpretabilidad.

### Selección de Características

Este apartado es esencial para el aprendizaje de un buen clasificador. Existen muchos algoritmos los cuales son sensibles a las variables que utilicemos, de modo que si usamos variables que no aportan información al clasificador perjudican al aprendizaje. 

Otra gran ventaja es que simplifican mucho el coste computacional de la clasificación, ya que reducimos la dimensionalidad del conjunto de datos y por tanto ahorramos tiempo de cómputo.

Por último, cabe mencionar también que esto mejora enormemente la interpretabilidad del problema ya que nos resume el modelo en variables que son muy importantes a la hora de aprender el clasificador y que por tanto tenemos que tener en cuenta y nos ayuda a descartar aquellas que son inútiles o aportan poca información de utilidad.


#### Correlación

Antes de realizar las diferentes estrategias pertinentes para buscar reducir la dimensionalidad de los datos, vamos a centrarnos en algo básico que se suele dar con mucha frecuencia en los conjuntos de datos, la correlación. 

Dos variables están correladas cuando existe una relación fuerte entre ambas, de modo que en la medida en la que cambia una, cambia también la otra. Esto conlleva a que ambas variables aporten una información similar al modelo de aprendizaje y por tanto a que sea inútil contemplar ambas. Por tanto, es esencial analizar la correlación que existe entre las variables para ver que variables pueden ser eliminadas.

Durante el análisis de datos perdidos, con el paquete mice hemos podido darnos cuenta de que existen variables correladas en el conjunto de datos, ya que dicho paquete no trata variables correladas y cuando se encuentra alguna de este tipo, las ignora. Por lo tanto, ahora realizaremos un estudio de correlación.

```{r correlacion gráfico}

corrMatrix = cor(na.omit(datosSinRuido[,-51]))
corrplot::corrplot(corrMatrix, type = "upper", order="FPC", tl.col = "black", tl.srt = 45)

```

Cabe destacar que existen distintos tipos de correlación: positiva, negativa y no correlación. 

En la gráfica, se representan con puntos de color azul con diferente intensidad las variables que tienen correlación positiva y con color rojo las que tienen correlación negativa, siendo 1 y -1 lo máximo respectivamente. El color blanco indica la no correlación. 

Por tanto, podemos ver que existe una alta correlación positiva en la gran mayoría de las variables y correlación negativa entre un par de variables también. Viendo las altas correlaciones existentes, si aplicamos un umbral demasiado bajo seguramente eliminemos la gran mayoría de variables y por tanto perdamos mucha información que puede ser importante para el clasificador. Por tanto, aplicaremos un umbral bastante alto para que nos permitan eliminar algunas características que a priori sean redundantes pero que no nos hagan perder demasiada información.


```{r filtro correlacion}
set.seed(3)
#filtrado de variables:
altamenteCorreladas = caret::findCorrelation(corrMatrix, cutoff=0.99999)

datosSinCorrelacion = datosSinRuido [, -altamenteCorreladas]
datosSinCorrelacion = as_tibble(datosSinCorrelacion)

dim(datosSinCorrelacion)
```

Como podemos observar, hemos eliminado 13 variables de nuestro conjunto de datos, quedándonos con 37 características (más la de clase) lo que signfica que hemos conseguido reducir la dimensionalidad aproximadamente un 25%. Vamos a ver como quedaría ahora el gráfico de correlaciones.



```{r correlacion gráfico}

corrMatrix = cor(na.omit(datosSinCorrelacion[,-38]))

corrplot::corrplot(corrMatrix, type = "upper", order="FPC", tl.col = "black", tl.srt = 45)

```
Como podemos observar, siguen quedando variables muy correladas positivamente, sin embargo si quitamos el resto de variables muy correladas entre sí hemos podido comprobar que perdemos mucha información y por tanto empeora nuestro clasificador. También podemos observar que existe una alta correlación negativa entre las variables ''X12'' y ''X34'', sin embargo aunque mejoramos un poco el error en el conjunto del train, el test no mejora. Por tanto, pasaremos a aplicar técnicas de selección de características para intentar reducir aún más la dimensionalidad pero sin perder información.


```{r correlacion gráfico}

corrMatrix = cor(na.omit(datosTomek[,-51]))
set.seed(3)
#filtrado de variables:
altamenteCorreladas = caret::findCorrelation(corrMatrix, cutoff=0.99999)

datosSinCorrelacion = datosTomek [, -altamenteCorreladas]
datosSinCorrelacion = as_tibble(datosSinCorrelacion)

dim(datosSinCorrelacion)

```




#### Filter: Chi-Cuadrado

Calcula la dependencia de cada variable a la variable de clase utilizando el test estadístico de independencia de Chi-Cuadrado.

```{r chiquadrado}


# se calculan los pesos de los atributos: la medida devuelta
# indica el nivel de dependencia de cada atributo frente a la
# variable clase
weights <- FSelector::chi.squared(C~.,datosSinCorrelacion)
print(weights)

# se seleccionan los 5 mejores
subset <- FSelector::cutoff.k(weights,40)

# se muestran los seleccionados
f <- as.simple.formula(subset,"C")
print(f)
#Class ~ X17 + X1 + X16 + X2 + X6
#datosFChiSquared = datosSinCorrelacion[,-c(31)]
#datosFChiSquared
```

#### Filter: Correlation

Este algoritmo busca las variables más relevantes en términos de correlación con la variable de clase.

```{r correlationf}

set.seed(2)
# se calculan los pesos mediante correlacion lineal, La variable
# clase se llama medv
d = datosTomek
d$C = as.integer(datosTomek$C)
weights <- FSelector::linear.correlation(C~., d)

# se muestran los pesos
print(weights)

# se seleccionan los tres mejores
subset <- FSelector::cutoff.k(weights,38)
f1 <- as.simple.formula(subset,"C")
print(f1)
barplot(weights$attr_importance, names.arg = rownames(weights), las=2)

# se determinan los pesos mediante rank.correlation
weights <- FSelector::rank.correlation(C~.,d)

# se muestran los pesos
print(weights)

# se seleccionan los mejores
subset <- FSelector::cutoff.k(weights,33)
f2 <- as.simple.formula(subset,"C")
print(f2)
barplot(weights$attr_importance, names.arg = rownames(weights), las=2)


```

este es el que mejor va, con los 25 mejores: C ~ X16 + X23 + X27 + X42 + X18 + X3 + X12 + X34 + X26 + X44 + 
    X21 + X41 + X31 + X24 + X15 + X37 + X17 + X48 + X32 + X7 + 
    X30 + X1 + X47 + X20 + X13
    
    
Aplicando los dos métodos de correlación y atendiendo a las 10 mejores, podemos observar como existen variables que se repiten en ambos conjuntos y por tanto podemos decir que serán esenciales a la hora de aprender nuestro clasificador: "X16" y "X23"

#### Filter: Entropía

Este algoritmo busca las variables más relevantes en términos de correlación con la variable de clase. Se utilizan métodos basados en entropía.

```{r entropyf}

# se obtienen las medidas mediante ganancia de informacion
weights <- FSelector::information.gain(C~., datosSinRuido)

# se muestran los pesos y se seleccionan los mejores
print(weights)
subset <- FSelector::cutoff.k(weights,4)
f1 <- as.simple.formula(subset,"C")
print(f1)

# igual, pero con ganancia de informacion
weights <- FSelector::gain.ratio(C~., datosSinRuido)
print(weights)
subset <- FSelector::cutoff.k(weights,45)
f2 <- as.simple.formula(subset,"C")
print(f2)
# e igual con symmetrical.uncertainty

weights <- FSelector::symmetrical.uncertainty(C~., datosSinCorrelacion)
print(weights)
subset <- FSelector::cutoff.k(weights,35)
f3 <- as.simple.formula(subset,"C")
print(f3)

```

Hemos conseguido mejorar la precisión del train a 0.9810496 con C4.5, sin embargo no hemos logrado mejorar la precisión del test.


#### Filter: OneR
 
 ---COMPLETAR----
Este algoritmo busca las variables más relevantes en términos de correlación con la variable de clase. Se utilizan métodos basados en entropía.

```{r oneR}

# se calculan los pesos
weights <- FSelector::oneR(C~.,datosSinRuido)

# se muestran los resultados
print(weights)
subset <- FSelector::cutoff.k(weights,50)
f <- as.simple.formula(subset,"C")
print(f)


```

Con 35 conseguimos una precisión idéntica a la que teníamos inicialmente con 38.

```{r rf}
# se calculan los pesos
weights <- FSelector::random.forest.importance(C~.,datosSinCorrelacion, importance.type=1)

# se muestran los resultados
print(weights)
subset <- cutoff.k(weights,24)
f <- as.simple.formula(subset,"C")
print(f)
```

## Árboles de decisión 

Los algoritmos típicos para árboles sin utilizar multiclasificadores son:
  - Árboles utilizando el criterio GINI a la hora de decidir como hacer los cortes.
  - Algoritmo ID3 el cual utiliza como criterio la entropía basándose en Gain, el cual no esta implementado en R a priori por lo que en principio no lo usaremos.
  - Algoritmo C4.5 el cual utiliza como criterio la entropía basándose en el Gain Ratio, el cual mejora al anterior porque soluciona sus problemas y además trabaja bien con missing values.
  
Para comenzar, utilizaremos los tres algoritmos para obtener una referencia inicial de la tasa de error subiendola a kaggle. En primer lugar usaremos un tipo de arbol basando en GAIN, de la librería tree.

```{r tree with gain}

set.seed (2)

datos = datosSinRuido
# Construyo el arbol sobre el conjunto de entrenamiento
tree.train = tree(f3,datos)
#tree.train

summary(tree.train)

plot(tree.train)
text(tree.train, pretty=0)

#tree.train

# Aplico el arbol sobre el conjunto de test
tree.pred1 =predict(tree.train, test ,type ="class")

#Aplicamos cv para intentar podar el árbol búscando el mejor número de nodos hoja.

set.seed (3)
cv.train = cv.tree(tree.train ,FUN=prune.misclass )
names(cv.train )
cv.train

# Ahora podamos el arbol con prune.misclass
prune.train =prune.misclass (tree.train ,best = 5)
par(mfrow =c(1,1))
plot(prune.train)
text(prune.train ,pretty =0)


###Comprobar acierto en el train
tree.pred.train =predict (prune.train , datos ,type="class")
table(tree.pred.train ,datos$C)
precision(tree.pred.train ,datos$C)


# Comprobar acierto en el test
tree.pred=predict (prune.train , test ,type="class")
setwd("~/GitHub/practica_mineria_kaggle")
generarEnvio(tree.pred, path = "./decisionTrees/")

```

A priori obtenemos que las características más importantes son X1, X27, X23 Y X32. Sin embargo, por validación cruzada hemos obtenido el mismo error utilizando 4 nodos que 5, así que podaremos aquí el árbol y por tanto descartaremos la variable x32. Si quitamos el nodo X23 podemos ver que también nos siguen dando buenos resultados, sin embargo producen mayor error, por lo que probraremos con los dos. Con los 4 nodos hemos obtenido un error del 0.84737.




Ahora vamos a probar con el algoritmo C4.5 implementado en RWeka. Es interesante saber que en RWeka hay disponibles otres tres algoritmos de los cuales es posible utilizar solo dos de ellos (el otro es un multiclasificador).


```{r c4.5 ini}

set.seed (2)

datos = data.balanced.ou
## Comprobar acierto en el train
modelC4.5 = J48(C~., data=datos)
cv_resul = evaluate_Weka_classifier(modelC4.5,numFolds=10)
cv_resul

modelC4.5 = J48(C~., datos) 
precision(modelC4.5$predictions ,datos$C)

#Comprobar acierto en el test
modelC4.5.pred = predict(modelC4.5, test)
setwd("~/GitHub/practica_mineria_kaggle")
generarEnvio(modelC4.5.pred, path = "./decisionTrees/")

set.seed (2)

datos = datosSinCorrelacion
## Comprobar acierto en el train
modelC4.5 = LMT(C~., data=datos, control = Weka_control( M= 5))
cv_resul = evaluate_Weka_classifier(modelC4.5,numFolds=10, seed = 2)
cv_resul

#modelC4.5 = LMT(f1, datos) 
precision(modelC4.5$predictions ,datos$C)

#Comprobar acierto en el test
modelC4.5.pred = predict(modelC4.5, test)
setwd("~/GitHub/practica_mineria_kaggle")
generarEnvio(modelC4.5.pred, path = "./decisionTrees/")



```

Con control de weka:

Correctly Classified Instances        6525               94.2919 %
Incorrectly Classified Instances       395                5.7081 %
Kappa statistic                          0.862 
Mean absolute error                      0.0783
Root mean squared error                  0.2056
Relative absolute error                 19.0025 %
Root relative squared error             45.3093 %
Total Number of Instances             6920     

=== Confusion Matrix ===

    a    b   <-- classified as
 4700  213 |    a = 0
  182 1825 |    b = 1
[1] 0.9498555

Con todas las variables usando solo undersampling: 0.90301 en kaggle

Correctly Classified Instances        6520               93.9075 %
Incorrectly Classified Instances       423                6.0925 %
Kappa statistic                          0.8531
Mean absolute error                      0.0823
Root mean squared error                  0.2144
Relative absolute error                 19.8784 %
Root relative squared error             47.1304 %
Total Number of Instances             6943     

=== Confusion Matrix ===

    a    b   <-- classified as
 4694  219 |    a = 0
  204 1826 |    b = 1
[1] 0.9508858

usando correlaciones hemos obtenido exactamente lo mismo, por lo que hacemos más interpretable el modelo al reducir dimensionalidad.

=== 10 Fold Cross Validation ===

=== Summary ===

Correctly Classified Instances        6527               94.0084 %
Incorrectly Classified Instances       416                5.9916 %
Kappa statistic                          0.8555
Mean absolute error                      0.0846
Root mean squared error                  0.2149
Relative absolute error                 20.4487 %
Root relative squared error             47.2394 %
Total Number of Instances             6943     

=== Confusion Matrix ===

    a    b   <-- classified as
 4697  216 |    a = 0
  200 1830 |    b = 1
[1] 0.9477171


Con semilla 2 25 variables:




Semilla 2:

Correctly Classified Instances        6523               93.9507 %
Incorrectly Classified Instances       420                6.0493 %
Kappa statistic                          0.8558
Mean absolute error                      0.084 
Root mean squared error                  0.213 
Relative absolute error                 20.3013 %
Root relative squared error             46.8225 %
Total Number of Instances             6943     

=== Confusion Matrix ===

    a    b   <-- classified as
 4656  257 |    a = 0
  163 1867 |    b = 1
[1] 0.9481492

baja en kaggle a 0.90096

Con 25, 26, 33, 35 variables en correlacion pero empeora.
=== 10 Fold Cross Validation ===

=== Summary ===

Correctly Classified Instances        6523               93.9507 %
Incorrectly Classified Instances       420                6.0493 %
Kappa statistic                          0.8556
Mean absolute error                      0.0848
Root mean squared error                  0.2134
Relative absolute error                 20.5018 %
Root relative squared error             46.9187 %
Total Number of Instances             6943     

=== Confusion Matrix ===

    a    b   <-- classified as
 4660  253 |    a = 0
  167 1863 |    b = 1
[1] 0.9474291

Con 35 variables y correlación

=== 10 Fold Cross Validation ===

=== Summary ===

Correctly Classified Instances        6527               94.0084 %
Incorrectly Classified Instances       416                5.9916 %
Kappa statistic                          0.8553
Mean absolute error                      0.0853
Root mean squared error                  0.215 
Relative absolute error                 20.6074 %
Root relative squared error             47.2584 %
Total Number of Instances             6943     

=== Confusion Matrix ===

    a    b   <-- classified as
 4702  211 |    a = 0
  205 1825 |    b = 1
[1] 0.9510298
---Hemos obtenido un error de 0.85400, por lo cual hemos mejorado un poco el anterior pero hemos empeorado nuestra marca anterior. 

=== 10 Fold Cross Validation ===

=== Summary ===

Correctly Classified Instances        6527               94.0084 %
Incorrectly Classified Instances       416                5.9916 %
Kappa statistic                          0.8555
Mean absolute error                      0.0846
Root mean squared error                  0.2149
Relative absolute error                 20.4487 %
Root relative squared error             47.2394 %
Total Number of Instances             6943     

=== Confusion Matrix ===

    a    b   <-- classified as
 4697  216 |    a = 0
  200 1830 |    b = 1
[1] 0.9477171

###
Notas de subidas después de preprocesar:
1. Hemos obtenido mejor puntuación después de hacer la imputación con amelia con el método ctree, ya que C4.5 ya trata de por si los missing values ignorandolo y baja a 0.83. Hemos obtenido un accuracy de: 0.86064 
2. Con C4.5 hemos obtenido una mejor solución con los imputados del mice, sin embargo con ctree sigue dándonos mejor resultado con camelia.
3. Con el paquete RobCompositions con ctree hemos obtenido el mismo resultado que para amelia, mientras que con C4.5  hemos conseguido un valor de 0.85706 lo que mejora el anterior algoritmo. Por tanto será interesante seguir con estos resultados.
4. Con ctree después de haber eliminado las instancias con ruido hemos conseguido un acierot del 0.86 pero con c4.5 hemos mejorado hasta casi el 0.87. Solo hemos probado con las de robcompositions.
5. Con ctree después de haber eliminado el ruido nos encontramos que el algoritmo no supera el 0.86. Sin embargo con Amelia y C4.5 hemos podido comprobar que mejora hasta 0.87850 lo que nos daría la mejor puntuación hasta el momento conseguida.

6. Por lo que podemos observar, eliminar las variables altamente correladas no tiene mucho sentido ya que baja enormemente la precisión del clasificador. Sin embargo, puede que esto provoque un sobreajuste excesivo o que realmente

7. Con 0.99999 y 38 variables siendo una de clase, el algoritmo mejora c4.5 a 0.88718. Para c4.5 en el train obtenemos un accuracy de 0.98025, (91.1344 usano cv ) y para ctree: 0.9029.
8. Quitando la variable 12 con correlación negativa mejora un poco el modelo en cuanto al train, sin embargo en el test no se mejora.

9. Sin quitar las variables correladas, usando como dataset el limpio sin ruido y aplicando el algoritmo de selección de variables symmetry (con 45 variables) hemos mejorado a 0.89127 el test en kaggle y a 0.987013 el train. (con 50 variables aumentamos aún más el train a 0.9878)

10. Hemos obtenido un 0.90096 utilizando todas las variables, solo eliminando el ruido y no quitando correlaciones, usando el algoritmo LMT. Los resultados de cv obtenidos  sobre el train son: 
Correctly Classified Instances        6979               92.4861 %
Incorrectly Classified Instances       567                7.5139 %

y de predicción normal: [1] 0.9349324

11. Eliminando correlaciones hemos mejorado aún más lo anterior.0.90199. Los resultados de cv son: 
Correctly Classified Instances        6986               92.5788 %
Incorrectly Classified Instances       560                7.4212 %

[1] 0.9313544

12. con EF y eliminando correlación hemos conseguido lo mismo que con iPF.
12.1 Con EF y 4.5: Correctly Classified Instances        6647               94.4042 %
Incorrectly Classified Instances       394                5.5958 % y 0.9936089 de precisión, sin embargo en kaggle baja.
12.2 Con EF y LMT: Correctly Classified Instances        6786               96.3784 %
Incorrectly Classified Instances       255                3.6216 %
y [1] 0.9809686 de precisión pero en kaggle no da todo lo bueno que podría. (0.89...)
12.3 con analisis de correlación EF y C4.5 empeora bastante.
12.4 Con correlación, EF y LMT: Correctly Classified Instances        6974               92.4198 %
Incorrectly Classified Instances       572                7.5802 %
0.9313544

QUE DA LO MISMO QUE USANDO IPF, HABRÁ QUE COMPROBAR A VER COMO SE COMPORTA EL RESTO?

13. Haciendo smote (oversampling) no hemos conseguido mejorar en absoluto el modelo. Correctly Classified Instances       13769               96.8966 %
Incorrectly Classified Instances       441                3.1034 %

Este modelo genera mucho sobre ajuste:  0.9973962, por lo que no tiene pinta que vayamos a poder mejorar el modelo con oversampling. En kaggle con correlaciones y utilizando IPF, LMT Y correlaciones hemos conseguido: 0.88259.

14. Haciendo undersampling parece que va algo mejor. Hemos aplicado tomek link, ipf, lmt y correlación hemos conseguido buenos resultados pero no hemos conseguido llegar al máximo: 

Correctly Classified Instances        5638               93.8259 %
Incorrectly Classified Instances       371                6.1741 %

[1] 0.9567316 y en kaggle 0.90045
14.1 Haciendo dos iteraciones hemos conseguido mejorar un poco los resultados:
  Correctly Classified Instances        6515               93.8355 %
  Incorrectly Classified Instances       428                6.1645 %
   0.9477171 y en kaggle 0.90301 lo que nos mejora el modelo actual de ipf lmt correlaciones de 0.90199. [1] 0.9486991
  
   
   14.2 Haciendo tres itreaciones parece que no sube  (0.90199) así que dejaremos de reducir instancias:
   
   Correctly Classified Instances        6374               93.694  %
  Incorrectly Classified Instances       429                6.306  %
   

15. utilizando ros bajamos la accuracy a 0.88871 sin correlación:

Correctly Classified Instances       10519               95.5578 %
Incorrectly Classified Instances       489                4.4422 %
[1] 0.9992733

Poor lo que podemos ver bastante sobre ajuste.

15.1 con correlación: también la cagamos así que pasaremos de over and under.

-----pasamos a hacer selección de características. Hemos decidido hasta ahora que los pasos seguidos son: amelia, ipf para ruido, undersampling, correlacion, LMT: los datos de referencia en cv son estos: 
Correctly Classified Instances        6515               93.8355 %
  Incorrectly Classified Instances       428                6.1645 %
   0.9477171 y en kaggle --0.90301-- lo que nos mejora el modelo actual de ipf lmt correlaciones de 0.90199. [1] 0.9486991
   -----
   
16. Usando future selection aplicando correlación con la variable de clase,(las 25 mejores) hemos conseguido mejorar a  0.90964 lo que nos ha dado bastante impulso.

Correctly Classified Instances        6395               94.0026 %
Incorrectly Classified Instances       408                5.9974 %

0.9532559

CON SEMILLA
== 10 Fold Cross Validation ===

=== Summary ===

Correctly Classified Instances        6518               94.1908 %
Incorrectly Classified Instances       402                5.8092 %
Kappa statistic                          0.8603
Mean absolute error                      0.0777
Root mean squared error                  0.2094
Relative absolute error                 18.864  %
Root relative squared error             46.1554 %
Total Number of Instances             6920     

=== Confusion Matrix ===

    a    b   <-- classified as
 4680  233 |    a = 0
  169 1838 |    b = 1
[1] 0.9518786

en Kaggle 0.90301
   
Por tanto parece que vamos en buen camino y tendremos que seguir tirando por aquí.

si subimos a 30 mejores bajamos en cv por lo que no probaremos en kaggle.

17. Si usamos las 26 mejores subimos el acierto de cv:

Correctly Classified Instances        6409               94.2084 %
Incorrectly Classified Instances       394                5.7916 %
Kappa statistic                          0.8626
Mean absolute error                      0.0797
Root mean squared error                  0.2104
Relative absolute error                 19.0214 %
Root relative squared error             45.9802 %

1] 0.9526679 pero baja en kaggle: 0.90352

18. Con 27 variables baja un poco y con 24 también. Lo dejaremos así.

19. Hemos probado con la otra opción de correlación pero no mejora, también hemos probado con las basadas en gain y tampoco mejor (simmetry). 

20. Con Random Forest y 35 pero no nos llega a mejorar el modelo,:

=== 10 Fold Cross Validation ===

=== Summary ===

Correctly Classified Instances        6510               94.0751 %
Incorrectly Classified Instances       410                5.9249 %
Kappa statistic                          0.8567
Mean absolute error                      0.0802
Root mean squared error                  0.2102
Relative absolute error                 19.4833 %
Root relative squared error             46.3184 %
Total Number of Instances             6920     

[1] 0.9572254

Con 32 variables:

correctly Classified Instances        6529               94.3497 %
Incorrectly Classified Instances       391                5.6503 %
Kappa statistic                          0.8631
Mean absolute error                      0.0801
Root mean squared error                  0.2061
Relative absolute error                 19.4383 %
Root relative squared error             45.4291 %
Total Number of Instances             6920     

[1] 0.9562139

Con 30 variables: 

=== 10 Fold Cross Validation ===

=== Summary ===

Correctly Classified Instances        6534               94.422  %
Incorrectly Classified Instances       386                5.578  %
Kappa statistic                          0.8651
Mean absolute error                      0.0773
Root mean squared error                  0.2078
Relative absolute error                 18.7753 %
Root relative squared error             45.7835 %
Total Number of Instances             6920     

[1] 0.9602601

con 25 variables:

=== 10 Fold Cross Validation ===

=== Summary ===

Correctly Classified Instances        6532               94.3931 %
Incorrectly Classified Instances       388                5.6069 %
Kappa statistic                          0.8648
Mean absolute error                      0.0768
Root mean squared error                  0.2045
Relative absolute error                 18.6539 %
Root relative squared error             45.0577 %
Total Number of Instances             6920     

baja muchisimo.

=== Confusion Matrix ===

 4695  218 |    a = 0
  170 1837 |    b = 1
[1] 0.9507225
Con 24 variables: 

=== 10 Fold Cross Validation ===

=== Summary ===

Correctly Classified Instances        6536               94.4509 %
Incorrectly Classified Instances       384                5.5491 %
Kappa statistic                          0.8664
Mean absolute error                      0.0779
Root mean squared error                  0.2073
Relative absolute error                 18.9248 %
Root relative squared error             45.6889 %
Total Number of Instances             6920     

[1] 0.9543353

Con 26 variables: 

Correctly Classified Instances        6534               94.422  %
Incorrectly Classified Instances       386                5.578  %
Kappa statistic                          0.8655
Mean absolute error                      0.0777
Root mean squared error                  0.2056
Relative absolute error                 18.8767 %
Root relative squared error             45.3151 %
Total Number of Instances             6920     

=== Confusion Matrix ===
[1] 0.9546243

Con 23 variables: 

=== Summary ===

Correctly Classified Instances        6529               94.3497 %
Incorrectly Classified Instances       391                5.6503 %
Kappa statistic                          0.8637
Mean absolute error                      0.0808
Root mean squared error                  0.2068
Relative absolute error                 19.6189 %
Root relative squared error             45.5685 %
Total Number of Instances             6920     
[1] 0.9536127

Con 24 variables se queda cerca de mejorar en kaggle: 0.90505 habría que probar con 25 variables.



-Habría que probar usando como importancia el tipo 2 en vez del 1(el que usa el gain)

probando con 25 variables con gain:

Correctly Classified Instances        6536               94.4509 %
Incorrectly Classified Instances       384                5.5491 %
Kappa statistic                          0.866 
Mean absolute error                      0.0773
Root mean squared error                  0.2045
Relative absolute error                 18.7579 %
Root relative squared error             45.0714 %
Total Number of Instances             6920     

[1] 0.9539017

Con 26 variables: baja en kaggle : 0.90045, pero va bastante bien

=== 10 Fold Cross Validation ===

=== Summary ===

Correctly Classified Instances        6545               94.5809 %
Incorrectly Classified Instances       375                5.4191 %
Kappa statistic                          0.869 
Mean absolute error                      0.0765
Root mean squared error                  0.2033
Relative absolute error                 18.5708 %
Root relative squared error             44.8046 %
Total Number of Instances             6920     

[1] 0.9536127
Con 27 variables empeora.



21. Probar modelos Wrapper like Boruta
```{r boruta}

# aprende el modelo
Bor.son <- Boruta(C~.,data=datosTomek,doTrace=2)

# muestra los resultados
print(Bor.son)

# se ven los resultados de decision de cada variable
print(Bor.son$finalDecision)

# imprime las estadisticas
stats <- attStats(Bor.son)
print(stats)

# se muestran los resultados en forma grafica
plot(Bor.son)

# muestra un grafico de los resultado: los valores en
# rojo estan relacionados con las variables confirmadas
# mientras que los verdes con variables descartadas
plot(normHits~meanImp,col=stats$decision,data=stats)


```
```{r bor_rf}

library(randomForest)
# aplicacion del metodo de seleccion
Bor.ozo <- Boruta(C~.,data=datosTomek,doTrace=2)

cat("Random forest sobre todos los atributos\n")
model1 <- randomForest(C~.,data=datosTomek)
print(model1)

cat("Random forest unicamente sobre atributos confirmados\n")
model2 <- randomForest(datosTomek[,getSelectedAttributes(Bor.ozo)],datosTomek$C)
print(model2)

# se muestra un grafico con los resultados de la seleccion
plot(Bor.ozo)


```

Después de ejecutar el análisis de boruta, hemos podido comprobar que todas las variables son importantes para el modelo desde su punto de vista. Sin embargo, los mejores resultados los hemos obtenido descartando variables con correlaciones muy muy altas y utilizando las 25 variables que más correlación tienen con la variable de salida.

Probaremos ahora con otros métodos, pero parece que no vamos a conseguir una reducción de variables interesantes:


22. Probar a toquetear el modelo.



